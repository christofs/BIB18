[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BIB18",
    "section": "",
    "text": "BIB18 – Curation and analyses of the dataset derived from the XVIIIe: Bibliographie"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "BIB18",
    "section": "Overview",
    "text": "Overview\nSome analyses of the dataset are available on the present website. The analyses are based on the RDF version of the data (currently, v0.1.0, May 20, 2023) and have been performed by Christof Schöch. Note that all numbers can be expected to shift slighly as the process of cleaning the data proceeds. Collaborative authorship / editorship as well as the publication types will most likely be most affected by corrections. The scripts used to create the analyses are integrated into the pages on this website for inspection (click on Code), but are also available as Jupyter Notebooks at github.com/christofs/BIB18.\nSource of the data: Benoît Melançon. For background information see his “Libération des données” and for the corresponding Dataverse deposit, see doi.org/10.5683/SP3/PYYEEH.\nFor more information on the transformation process, and to obtain the dataset in various formats (BibTeX, Zotero RDF, JSON), see the Github repository github.com/christofs/BIB18. The original transformation was performed in May 2023.\nThe full bibliography can also be consulted on Zotero, see: zotero.org/groups/5067408/BIB18."
  },
  {
    "objectID": "index.html#some-remarks-on-the-data",
    "href": "index.html#some-remarks-on-the-data",
    "title": "BIB18",
    "section": "Some remarks on the data",
    "text": "Some remarks on the data\n\nFor publication dates, only the year has been used.\nCorrect ordering of first and last names of authors or publishers is difficult, so errors may persist.\nThe data source does not distinguish between URLs and DOIs; consequently, they are all listed in the URL field. However, few DOIs are present anyway.\nInformation on the language of a publication has been added automatically, using the lingua-py module. This information can be erroneous in some cases (and will be improved by hand).\nAfter transformation into BibTex, the BibTeX-tidy tool was used: https://flamingtempura.github.io/bibtex-tidy.\nThe resulting files are available in the archive folder and form the basis for import into Zotero.\nSeveral Zotero export formats are available in the data folder.\nFuture corrections to the data will be made on Zotero, with updates to the formats in the datafolder."
  },
  {
    "objectID": "index.html#nota-bene",
    "href": "index.html#nota-bene",
    "title": "BIB18",
    "section": "Nota bene",
    "text": "Nota bene\nBenoît Melançon specifies: “Quiconque souhaite s’approprier ces données peut le faire, sous deux conditions. 1. L’attribution de la collecte des données doit toujours être rappelée, par exemple sous la forme «Données colligées par Benoît Melançon». 2. Aucune exploitation commerciale de ces données n’est tolérée. Elles ne peuvent pas être vendues sous quelque forme que ce soit. Autrement dit, chez Creative Commons : Attribution-NonCommercial 4.0 International (CC BY-NC 4.0) — https://creativecommons.org/licenses/by-nc/4.0/.”"
  },
  {
    "objectID": "index.html#citation-suggestion",
    "href": "index.html#citation-suggestion",
    "title": "BIB18",
    "section": "Citation suggestion",
    "text": "Citation suggestion\nChristof Schöch. BIB18 – Curation and Analysis of the dataset derived from the XVIIIe: Bibliographie, v0.3.0. Github.com, 2023. URL: https://christofs.github.io/BIB18/, DOI: 10.5281/zenodo.8166110."
  },
  {
    "objectID": "index.html#notes-on-work-in-progress",
    "href": "index.html#notes-on-work-in-progress",
    "title": "BIB18",
    "section": "Notes on work in progress",
    "text": "Notes on work in progress\nThis website is work in progress. All bugs, todos, ideas for improvement are listed in the issue tracker. Feel free to add issues if you notice something!"
  },
  {
    "objectID": "coauthors.html",
    "href": "coauthors.html",
    "title": "Co-authors",
    "section": "",
    "text": "This page focuses on co-authorship in the XVIIIe siècle: Bibliographie.\nCode\n# === Imports === \n\nimport re \nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom os.path import join, abspath, normpath, realpath\nimport os\nfrom lxml import etree\nfrom io import StringIO, BytesIO\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\nfrom IPython.display import HTML, display   \nimport networkx as nx\nfrom pyvis.network import Network \nimport matplotlib.pyplot as plt\n\n\n# === Namespaces === \n\nnamespaces = {\n    \"foaf\" : \"http://xmlns.com/foaf/0.1/\",\n    \"bib\" : \"http://purl.org/net/biblio#\",\n    \"dc\" : \"http://purl.org/dc/elements/1.1/\",\n    \"z\" : \"http://www.zotero.org/namespaces/export#\",\n    \"rdf\" : \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n    }\n\n\n# === Files and parameters === \n\nwdir = join(\"/\", \"media\", \"christof\", \"Data\", \"Github\", \"christofs\", \"BIB18\")\nbibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF_2024-01-02.rdf\") \n#bibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF_TEST.rdf\") \n\n\n# === Load the dataset === \n\ndef read_xml(bibdatafile): \n    bibdata = etree.parse(bibdatafile)\n    return bibdata\nbibdata = read_xml(bibdatafile)"
  },
  {
    "objectID": "coauthors.html#number-of-collaborators-in-co-authorship",
    "href": "coauthors.html#number-of-collaborators-in-co-authorship",
    "title": "Co-authors",
    "section": "Number of collaborators in co-authorship",
    "text": "Number of collaborators in co-authorship\nThis section determines how frequent co-authorship is in the dataset. This is based on the number of names found within the “authors” element. For a comparison with co-editorship, see the section Co-editors.\n\n\nCode\ndef get_coauthors(bibdata): \n    \"\"\"\n    Extracts the authorship information from the data. \n    \"\"\"\n    # First, find all lists of authors\n    xpath = \"//bib:authors\"\n    authors = bibdata.xpath(xpath, namespaces=namespaces)\n    print(\"There are \" + str(len(authors)) +  \" publications with a list of one or several authors in the dataset.\")\n\n    # Then, for each publication, get the list of authors (first and last name)\n    coauthors = []\n    xpath_lastname = \".//foaf:surname/text()\"\n    xpath_firstname = \".//foaf:givenName/text()\"\n    for item in authors: \n        lastnames = item.xpath(xpath_lastname, namespaces=namespaces)\n        firstnames = item.xpath(xpath_firstname, namespaces=namespaces)\n        fullnames = []\n        for last, first in zip(lastnames, firstnames):\n            full = last + \"_\" + first\n            fullnames.append(full)\n        if fullnames: \n            coauthors.append(fullnames)\n    return coauthors\ncoauthors = get_coauthors(bibdata)\n\n\nThere are 56839 publications with a list of one or several authors in the dataset.\n\n\nAs the next step, we want to see how many authors publications in the bibliography typically have, expressed both in counts and in percentages. The table below shows the results.\n\n\nCode\ndef get_coauthordata(coauthors): \n\n    # Get count of coauthor numbers\n    count_coauthors = Counter([len(item) for item in coauthors])\n\n    # Turn into a dataframe\n    coauthordata = pd.DataFrame.from_dict(\n        count_coauthors,\n        orient=\"index\",\n        columns=[\"count\"])\n\n    # Calculate percentages\n    coauthors_sum = np.sum(coauthordata[\"count\"])\n    coauthordata[\"percentage\"] = np.round(np.multiply(coauthordata[\"count\"] / coauthors_sum, 100), 2)\n\n    # Prettify the DataFrame\n    coauthordata[\"number\"] = coauthordata.index\n    coauthordata = coauthordata.sort_values(by=\"number\")\n    coauthordata = coauthordata[['number', 'count', 'percentage']]\n    display(HTML(coauthordata.to_html(index=False)))\n\n    return coauthordata\ncoauthordata = get_coauthordata(coauthors)\n\n\n\n\n\nnumber\ncount\npercentage\n\n\n\n\n1\n52635\n94.16\n\n\n2\n3244\n5.80\n\n\n3\n13\n0.02\n\n\n4\n5\n0.01\n\n\n\n\n\nOf course, we can also display these results as a bar chart, for easier visual inspection (and comparison to the barchart in the Co-editor section.)\n\n\nCode\ndef visualize_coauthordata(coauthordata): \n    \"\"\"\n    Create a simple bar plot that shows \n    the percentage of each count of co-authors in the dataset.\n    \"\"\"\n\n    import seaborn.objects as so\n    (\n        so.Plot(data=coauthordata, x=\"number\", y=\"count\", text=\"percentage\")\n        .add(so.Bar())\n        .add(so.Text(color=\"w\", valign=\"top\", offset=5))\n        .scale(x=so.Continuous().tick(every=1))\n        .label(\n            x=\"Number of co-authors\",\n            y=\"Counts (% shown as labels)\",\n            title = \"Counts and percentages of different number of co-authors\")\n        .save(\"figures/coauthor-percentages.png\", dpi=300)\n        .show()\n    )\nvisualize_coauthordata(coauthordata)\n\n\n\n\n\nRegarding authorship (of books, articles and chapters), the figure above shows that single authorship is clearly the norm in this dataset (with around 94% of the cases). The remaining 6% are alsmost all cases of dual authorship, with a rare exceptions of triple or more authorship.\nIn contrast, when looking at editorship, joint editorship with two editors is the most common case (around 42%), closely followed by single editorship (almost 40%). Triple co-editorship is rarer, but not uncommon (at around 14%)."
  },
  {
    "objectID": "coauthors.html#coauthor-pairs",
    "href": "coauthors.html#coauthor-pairs",
    "title": "Co-authors",
    "section": "Coauthor pairs",
    "text": "Coauthor pairs\nEven if coauthorship is comparatively rare, we can still try to build a network of co-authors. The table below shows the people most frequently authoring a publication jointly.\n\n\nCode\nimport warnings\nimport pandas as pd\nfrom pandas.core.common import SettingWithCopyWarning\nwarnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n\n\ndef network_coauthors(coauthordata): \n    \"\"\"\n    Builds the network of people having collaborated as authors. \n    \"\"\"\n\n    # Filter out the single author data\n    coauthors = [item for item in coauthordata if len(item) &gt; 1]\n\n    # Establish the count of each collaboration between authors\n    import itertools \n    all_coauthor_combinations = []\n    for item in coauthors: \n        coauthors_combinations = list(itertools.combinations(item, 2))\n        coauthors_combinations = [tuple(sorted(item)) for item in coauthors_combinations]\n        for coauthcomb in coauthors_combinations: \n            all_coauthor_combinations.append(coauthcomb)\n    cac = dict(Counter(all_coauthor_combinations)) # ccc = coeditor_combinations_count\n\n    # Transform to a DataFrame\n    cac = pd.DataFrame.from_dict(cac, orient=\"index\", columns=[\"count\"])\n    cac = cac.reset_index()\n    cac_split = pd.DataFrame(cac[\"index\"].tolist())\n    cac_merged = cac_split.merge(cac, left_index=True, right_index=True)\n    cac = cac_merged.drop([\"index\"], axis=1)\n    cac = cac.rename({0 : \"coauthor1\", 1 : \"coauthor2\"}, axis=1)\n    cac = cac.sort_values(by=\"count\", ascending=False)\n    display(HTML(cac.head(15).to_html(index=False)))\n\n    return cac\ncac = network_coauthors(coauthors)\n\n\n\n\n\ncoauthor1\ncoauthor2\ncount\n\n\n\n\nAlbertan_Christian\nChouillet_Anne-Marie\n22\n\n\nDesberg_Stephen\nMarini_Enrico\n15\n\n\nBerchtold_Jacques\nPorret_Michel\n12\n\n\nCabane_Franck\nChouillet_Anne-Marie\n10\n\n\nBoussuge_Emmanuel\nLaunay_Françoise\n9\n\n\nHourcade_Philippe\nVinha_Mathieu Da\n9\n\n\nBrown_Andrew\nKölving_Ulla\n8\n\n\nChouillet_Anne-Marie\nLoty_Laurent\n8\n\n\nAdelson_Robert\nLetzter_Jacqueline\n7\n\n\nLüsebrink_Hans-Jürgen\nReichardt_Rolf\n7\n\n\nBonpland_Aimé\nvon Humboldt_Alexander\n7\n\n\nPastoureau_Michel\nWagman_Mathilde\n7\n\n\nGengembre_Gérard\nGoldzink_Jean\n7\n\n\nChouillet, Anne-Marie_Irène Passeron\nPrin_François\n6\n\n\nMcKenna_Antony\nMori_Gianluca\n6"
  },
  {
    "objectID": "coauthors.html#co-author-networks",
    "href": "coauthors.html#co-author-networks",
    "title": "Co-authors",
    "section": "Co-author networks",
    "text": "Co-author networks\nBased on the coauthor pairs, we can now build a network visualisation.\n\n\nCode\ndef create_plot(cac, topn): \n    \"\"\" \n    Plot the co-editor data as a network using pyvis. \n    \"\"\"\n    \n    # Filter the dataset \n    # Determine the top N most frequent co-editors\n    coauthors_top = list(set(list(cac.head(topn).loc[:,\"coauthor1\"]) +\\\n        list(cac.head(topn).loc[:,\"coauthor2\"])))\n\n    print(\"Among all authors, \" + str(len(coauthors_top)) + \" have been selected as the most active co-authors.\")\n\n    # Filter the DataFrame to include just the collaborations involving at least one of the top co-authors. \n    # The resulting DataFrame will have all collaborations between the top co-editors and their co-authors. \n    cac = cac[(cac[\"coauthor1\"].isin(coauthors_top)) |\\\n                       (cac[\"coauthor2\"].isin(coauthors_top))]\n\n    # Simplify the labels \n    cac.loc[:,'coauthor1'] =  [re.sub(r', .*','', str(x)) for x in cac.loc[:,'coauthor1']]\n    cac.loc[:,'coauthor2'] =  [re.sub(r', .*','', str(x)) for x in cac.loc[:,'coauthor2']]\n\n    # Prepare the dataset\n    cac.rename(columns={\"count\": \"weight\"}, inplace=True)\n    cac = cac[cac[\"weight\"] &gt; 0] # for filtering, with \"0\", all items remain\n\n    # Load the data into a NetworkX graph\n    net = Network('1600px', '2800px', notebook=True, cdn_resources='in_line')\n    G = nx.Graph()\n    for line in cac.iterrows(): \n        #print(line[1][0], end=\"\\n\")\n        G.add_edge(\n            # full names\n            line[1][0],\n            line[1][1],\n            # last names only\n            #re.split(\"_\", line[1][0])[0],\n            #re.split(\"_\", line[1][1])[0],\n            weight=line[1][2],\n            title=line[1][2],\n            )\n    degrees = dict(G.degree)\n    degrees.update((x, ((y*50)**0.5)) for x, y in degrees.items())\n    nx.set_node_attributes(G, degrees, 'size') \n    print(\"With all their co-authors, the network includes a total of\", str(nx.number_of_nodes(G)), \"authors.\")\n\n    # Plot the data using pyvis\n    net.from_nx(G)\n    net.toggle_physics(True)\n    net.show(\"figures/coauthor-network.html\")\ncreate_plot(cac, topn=100)\n\n\nAmong all authors, 183 have been selected as the most active co-authors.\nWith all their co-authors, the network includes a total of 354 authors.\nfigures/coauthor-network.html\n\n\nClick on the following plot to load the interactive, zoomable and scrollable network.\n\n\n\n\n\nAt a first glance, especially when compared to the dense co-editor network, what strikes here is the relative sparsity of the connections. There is a more densely-connected subnetwork at the center, but there are also many unconnected subnetworks of co-authorship."
  },
  {
    "objectID": "coeditors.html",
    "href": "coeditors.html",
    "title": "Co-editors",
    "section": "",
    "text": "This page focuses on co-editorship, especially of edited volumes, but including also editions, in the Bibliographie.\nCode\n# === Imports === \n\nimport re \nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom os.path import join, abspath, normpath, realpath\nimport os\nfrom lxml import etree\nfrom io import StringIO, BytesIO\nfrom collections import Counter\nimport pandas as pd\nimport netgraph\nimport numpy as np\nfrom IPython.display import HTML, display   \nimport networkx as nx\nfrom pyvis.network import Network \nimport matplotlib.pyplot as plt\n\n\n# === Namespaces === \n\nnamespaces = {\n    \"foaf\" : \"http://xmlns.com/foaf/0.1/\",\n    \"bib\" : \"http://purl.org/net/biblio#\",\n    \"dc\" : \"http://purl.org/dc/elements/1.1/\",\n    \"z\" : \"http://www.zotero.org/namespaces/export#\",\n    \"rdf\" : \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n    }\n\n\n# === Files and parameters === \n\nwdir = join(\"/\", \"media\", \"christof\", \"Data\", \"Github\", \"christofs\", \"BIB18\")\nbibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF_2024-01-02.rdf\") \n#bibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF_TEST.rdf\") \n#graphviz_file = join(wdir, \"figures\", \"collaboration_coeditors-network.svg\") \n\n\n# === Load the dataset === \n\ndef read_xml(bibdatafile): \n    bibdata = etree.parse(bibdatafile)\n    return bibdata\n\nbibdata = read_xml(bibdatafile)"
  },
  {
    "objectID": "coeditors.html#collecting-all-publications-that-have-editors",
    "href": "coeditors.html#collecting-all-publications-that-have-editors",
    "title": "Co-editors",
    "section": "Collecting all publications that have editors",
    "text": "Collecting all publications that have editors\nFor the time being, only books (including editions) and book sections are considered. Multiple entries of the same edited volume are taken into account only once. This avoids an undue inflation of the number of cases of co-editorship in cases where multiple chapters from the same edited volume are present in the database.\nNote that there is a peculiar practice in the French publishing landscape, that of edited volumes that have been edited (‘dirigé’) by one or several people in collaboration with one or several other people (‘avec la collaboration de’). The role of these latter collaborators, whether as scientific co-editors with a slightly lesser degree of responsibility than the main editor, or rather as editorial assistants, is not always clear. In addition, they are sometimes named on the front page, sometimes only in the front matter. Finally, this information is not represented clearly in the dataset at this point, so some caution is advised.\n\n\nCode\ndef get_editordata(bibdata): \n    \"\"\"\n    Collects data on the titles and editors of all collected volumes for which the dataset includes book sections (like chapters).\n    Simplifies the data to just unique titles of collected volumes with their editors.\n    \"\"\"\n    # Find all instances of book chapters or book sections\n    xpath = \"//bib:BookSection\"\n    sections = bibdata.xpath(xpath, namespaces=namespaces)\n    print(\"There are \" + str(len(sections)) +  \" instances of element 'BookSection'.\")\n\n    # Also, find all instances of books mentioned as a whole and that have editors. \n    xpath = \"/rdf:RDF/bib:Book\"\n    books = bibdata.xpath(xpath, namespaces=namespaces)\n    print(\"There are \" + str(len(books)) +  \" instances of element 'book'.\")\n    sections.extend(books)\n    print(\"Overall, there are\", len(sections), \"publications included at this point.\")\n\n    # For each section (and each book), get the editor(s) and booktitle\n    editordata = {}\n    counter = 0\n    for item in sections[:]: \n        counter +=1\n        # Get the editors' last names\n        xpath = \".//bib:editors//foaf:surname//text()\"\n        editors_last = item.xpath(xpath, namespaces=namespaces)\n        editors_last = [editor for editor in editors_last if \"lacollaborationde\" not in editor]\n        # Get the editors' first names\n        xpath = \".//bib:editors//foaf:givenName//text()\"\n        editors_first = item.xpath(xpath, namespaces=namespaces)\n        editors_first = [editor for editor in editors_first if \"avec\" not in editor]\n        # Combine the names in a concise manner\n        editors = []\n        for last, first in zip(editors_last, editors_first):\n            full = last + \"_\" + first\n            editors.append(full)\n        # Get the book titles\n        xpath = \".//bib:Book/dc:title//text()\"\n        try: \n            booktitle = item.xpath(xpath, namespaces=namespaces)[0]\n        except: \n            booktitle = \"missing title number \" + str(counter)\n            #print(booktitle) # Only happens less than 10 times, it seems. \n        # Collect each booktitle only once; only last occurrence is kept (!)\n        if len(editors) &gt; 0: \n            editordata[booktitle] = editors\n    # Check results \n    #print(\"All of the book sections mentioned above correspond to (only)\", len(editordata), \"different titles of edited volumes.\")\n    #ratio = np.round(np.divide(len(sections), len(editordata)),2)\n    #print(\"This means that, on average, an edited volume is mentioned\", ratio, \"times, each mention corresponding to one section of the volume being mentioned.\")\n\n    #for title,editors in editordata.items(): \n    #    print(editors,title)\n    return editordata\neditordata = get_editordata(bibdata)\n\n\nThere are 14421 instances of element 'BookSection'.\nThere are 23059 instances of element 'book'.\nOverall, there are 37480 publications included at this point.\n\n\nNote that many of the books don’t have editors, because they are monographs. Also, many book sections correspond to different chapters from the same edited volume. Whenever that is the case, the books without editor are discarded and each edited volume is only considered once, in the next step. The total number of publications considered for the co-editor analysis is therefore substantially lower."
  },
  {
    "objectID": "coeditors.html#prevalence-of-co-editor-numbers",
    "href": "coeditors.html#prevalence-of-co-editor-numbers",
    "title": "Co-editors",
    "section": "Prevalence of co-editor numbers",
    "text": "Prevalence of co-editor numbers\nBased on the data on book titles that have one or several editors, we can now investigate the prevalence of the different numbers of editors books typically have. The following table shows this data.\n\n\nCode\ndef get_coeditor_numbers(editordata): \n    \"\"\"\n    Based on the editordata, establish the number of times each number of co-editors appears, \n    as well as the corresponding percentages for each number of co-editors. \n    \"\"\"\n    coeditors = Counter([len(item) for item in editordata.values()])\n    coeditors = pd.DataFrame(pd.Series(coeditors, name=\"count\"))\n    #coeditors.drop(0, axis=0, inplace=True)\n\n    coeditors[\"percentage\"] = np.round(np.multiply(np.divide(coeditors[\"count\"], np.sum(coeditors[\"count\"])),100),1)\n    # Prettify the DataFrame\n    coeditors[\"number\"] = coeditors.index\n    coeditors = coeditors.sort_values(by=\"number\")\n    coeditors = coeditors[['number', 'count', 'percentage']]\n    display(HTML(coeditors.to_html(index=False)))\n    return coeditors\ncoeditors = get_coeditor_numbers(editordata)\n\n\n\n\n\nnumber\ncount\npercentage\n\n\n\n\n1\n2730\n39.8\n\n\n2\n2847\n41.5\n\n\n3\n966\n14.1\n\n\n4\n259\n3.8\n\n\n5\n49\n0.7\n\n\n6\n5\n0.1\n\n\n10\n1\n0.0\n\n\n\n\n\nThe following visualization shows counts and percentages for different numbers of co-editors.\n\n\nCode\ndef visualize_coeditor_numbers(coeditors): \n    \"\"\"\n    Create a simple bar plot that shows \n    the percentage of each number of co-editors in the dataset.\n    \"\"\"\n\n    coeditors = coeditors.iloc[:-1,:]\n    import seaborn.objects as so\n    (\n        so.Plot(data=coeditors, x=\"number\", y=\"count\", text=\"percentage\")\n        .add(so.Bar())\n        .add(so.Text(color=\"w\", valign=\"top\", offset=5))\n        .scale(x=so.Continuous().tick(every=1))\n        .label(\n            x=\"Number of co-editors\",\n            y=\"Counts (% shown as labels)\",\n            title = \"Counts and percentages of different number of co-editors\")\n        .save(\"figures/coeditor-percentages.png\", dpi=300)\n        .show()\n    )\nvisualize_coeditor_numbers(coeditors)\n\n\n\n\n\nCompared with the data on co-authorship (of books, articles and chapters), the figure above shows that co-editorship works quite differently. Indeed, when looking at editorship (of edited volumes or editions), joint editorship with two editors is the most common case (with almost 42% of the cases). Single editorship, however, is only slighly less widespread (at 39%). Triple co-editorship is also not uncommon, though clearly at a much lower level (at around 14%).\nIt could be interesting to distinguish between editorship of edited volumes on the one hand, and textual editions, on the other hand. This is future work, because the dataset does not distinguish these two publication types at the moment."
  },
  {
    "objectID": "coeditors.html#coeditor-pairs",
    "href": "coeditors.html#coeditor-pairs",
    "title": "Co-editors",
    "section": "Coeditor pairs",
    "text": "Coeditor pairs\nThis section looks at which people have frequently collaborated as editors of edited volumes and/or editions. The table below shows the 10 most active pairs of editors.\n\n\nCode\nimport warnings\nimport pandas as pd\nfrom pandas.core.common import SettingWithCopyWarning\nwarnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n\n\ndef get_recurring_coeditors(editordata, topn): \n    \"\"\"\n    Which people have frequently collaborated as editors? \n    This is based on the data collected above (editors of unique titles). \n    \"\"\"\n\n    # Define filenames for output. \n    coeditorcounts_top_file = join(wdir, \"results\", \"coeditor-counts_top.csv\")\n    coeditorcounts_full_file = join(wdir, \"results\", \"coeditor-counts_full.csv\")\n\n    # Get editor names from editordata. \n    coeditors = editordata.values()\n    print(\"Overall,\", len(coeditors), \"publications with editors are considered here.\")\n    coeditor_names = [item for sublist in coeditors for item in sublist]\n    print(\"Also, there are\", len(set(coeditor_names)) , \"different editors represented in the data.\")\n\n    # Establish the count of each collaboration between editors\n    import itertools \n    all_coeditor_combinations = []\n    for item in coeditors: \n        coeditor_combinations = list(itertools.combinations(item, 2))\n        coeditor_combinations = [tuple(sorted(item)) for item in coeditor_combinations]\n        for coedcomb in coeditor_combinations: \n            all_coeditor_combinations.append(coedcomb)\n    ccc = dict(Counter(all_coeditor_combinations)) # ccc = coeditor_combinations_count\n\n    # Transform to a DataFrame\n    ccc = pd.DataFrame.from_dict(ccc, orient=\"index\", columns=[\"count\"])\n    ccc = ccc.reset_index()\n    ccc_split = pd.DataFrame(ccc[\"index\"].tolist())\n    ccc_merged = ccc_split.merge(ccc, left_index=True, right_index=True)\n    ccc = ccc_merged.drop([\"index\"], axis=1)\n    ccc = ccc.rename({0 : \"coeditor1\", 1 : \"coeditor2\"}, axis=1)\n    ccc = ccc.sort_values(by=\"count\", ascending=False)\n    #print(ccc.head())\n    #print(ccc.shape, \"shape of dataframe\")\n    with open(join(coeditorcounts_full_file), \"w\", encoding=\"utf8\") as outfile: \n        ccc.to_csv(outfile, sep=\";\")\n\n    # Filter the DataFrame to make it manageable for visualization\n    # Determine the top N most frequent co-editors\n    coeditors_top = list(set(list(ccc.head(topn).loc[:,\"coeditor1\"]) +\\\n        list(ccc.head(topn).loc[:,\"coeditor2\"])))\n    #print(coeditors_top)\n    print(\"Among all editors, \" + str(len(coeditors_top)) + \" have been selected as the most active co-editors.\")\n    print(\"In the following analysis, all editors they have collaborated with, however, are included.\")\n    # Filter the DataFrame to include just the collaborations involving at least one of the top co-editors. \n    # The resulting DataFrame will have all collaborations between the top co-editors and their co-editors. \n    ccc_filtered = ccc[(ccc[\"coeditor1\"].isin(coeditors_top)) |\\\n                       (ccc[\"coeditor2\"].isin(coeditors_top))]\n    #print(ccc_filtered.shape, \"shape of dataframe of top co-editors and their co-editors.\")\n    # Simplify the labels \n    #ccc_filtered = ccc_filtered.replace(' .*?]', '',regex=True).astype(str)\n    ccc_filtered.loc[:,'coeditor1'] =  [re.sub(r', .*','', str(x)) for x in ccc_filtered.loc[:,'coeditor1']]\n    ccc_filtered.loc[:,'coeditor2'] =  [re.sub(r', .*','', str(x)) for x in ccc_filtered.loc[:,'coeditor2']]\n\n    # Display this as a formatted table    \n    display(HTML(ccc_filtered.head(10).to_html(index=False)))\n\n    # Save to disk \n    with open(join(coeditorcounts_top_file), \"w\", encoding=\"utf8\") as outfile: \n        ccc_filtered.to_csv(outfile, sep=\";\")\n    return ccc, ccc_filtered\n\nccc, ccc_filtered = get_recurring_coeditors(editordata, topn=150)\n\n\nOverall, 6857 publications with editors are considered here.\nAlso, there are 7151 different editors represented in the data.\nAmong all editors, 208 have been selected as the most active co-editors.\nIn the following analysis, all editors they have collaborated with, however, are included.\n\n\n\n\n\ncoeditor1\ncoeditor2\ncount\n\n\n\n\nHerman_Jan\nPelckmans_Paul\n24\n\n\nHasquin_Hervé\nMortier_Roland\n18\n\n\nBiard_Michel\nLeuwers_Hervé\n12\n\n\nBiard_Michel\nBourdin_Philippe\n11\n\n\nBerchtold_Jacques\nPorret_Michel\n9\n\n\nBourdin_Philippe\nLeuwers_Hervé\n8\n\n\nPeeters_Kris\nPelckmans_Paul\n8\n\n\nHerman_Jan\nPeeters_Kris\n8\n\n\nDidier_Béatrice\nNeefs_Jacques\n7\n\n\nBerchtold_Jacques\nMartin_Christophe\n7"
  },
  {
    "objectID": "coeditors.html#co-editor-network",
    "href": "coeditors.html#co-editor-network",
    "title": "Co-editors",
    "section": "Co-editor network",
    "text": "Co-editor network\nBased on the frequency of co-editorships, it is possible to draw a network representation.\nThe network visualization shows the editors as nodes, with their weighted degree determining the node size (the more publications they have edited with together with someone else, the larger the node). And it shows the intensity of their collaboration as the edges, with the number of coedited volumes determining the edge thickness (the more publications two editors have collaborated on, the thicker the line between them). The network layout aims to show groups of people who belong to a sub-network of co-editors who, as a group, work together frequently.\n\n\nCode\ndef create_plot(ccc_filtered): \n    \"\"\" \n    Plot the co-editor data as a network using pyvis. \n    \"\"\"\n    # Prepare the dataset\n    cccf = ccc_filtered\n    cccf.rename(columns={\"count\": \"weight\"}, inplace=True)\n    cccf = cccf[cccf[\"weight\"] &gt; 0] # for filtering, with \"0\", all items remain\n\n    # Load the data into a NetworkX graph\n    net = Network('1600px', '2800px', notebook=True, cdn_resources='in_line')\n    G = nx.Graph()\n    for line in cccf.iterrows(): \n        #print(line[1][0], end=\"\\n\")\n        G.add_edge(\n            # full names\n            line[1][0],\n            line[1][1],\n            # last names only\n            #re.split(\"_\", line[1][0])[0],\n            #re.split(\"_\", line[1][1])[0],\n            weight=line[1][2],\n            title=line[1][2],\n            )\n    degrees = dict(G.degree)\n    degrees.update((x, ((y*50)**0.5)) for x, y in degrees.items())\n    nx.set_node_attributes(G, degrees, 'size') \n    print(\"The network includes a total of\", str(nx.number_of_nodes(G)), \"editors.\")\n\n    # Plot the data using pyvis\n    net.from_nx(G)\n    net.toggle_physics(True)\n    net.show(\"figures/coeditor-network.html\")\ncreate_plot(ccc_filtered)\n\n\nThe network includes a total of 618 editors.\nfigures/coeditor-network.html\n\n\nThis data can be visualized as a network. Click on the following image for an interactive network plot.\nWhen interpreting the data, please note that (a) the number of key editors selected and (b) the minimum number of collaborations are important parameters of this network. If all editors are included, and/or single collaborations are retained, the network becomes unwieldy (and the graph loads slowly). However, the number of sub-networks, and their degree of connectivity among each other, varies strongly when these parameters are modified.\nThe current network uses a total of 618 editors (all 208 editors included in the top-150 editor pairs and all editors connected to them), and does not filter out any rarer collaborations. That’s only around 10% of the total number of editors in the dataset, but at this setting, the network remains readable and already loads quite slowly.\n\n\n\ncoeditor network\n\n\nThe visualization shows, like the tabular display, that Herman and Pelckmans (with 24 co-edited volumes the collaborated on!) and Hasquins and Mortier (with 18 edited volumes) are the most intense collaborators in terms of co-editorship.\nMore interestingly, however, the network visualisation shows that there are multiple, independent co-editor networks. Some are star-shaped and dominated by one editor working with many different co-editors (like the sub-networks around Grande or Seth). A few are dominated by a binary relationship (like the ones around Neefs and Didier, Hersant et Ramond, Plagnol-Diéval and Cook, or the one made up of Mortier and Hasquin). Others involve a triangular relationship (like the ones involving Herman, Pelckmans, Peeters; or the one with Bourdin, Biard, Leuwers). The unconnected sub-network formed by five co-editors (Bouvier, Brunat, Kohlhauer, Barthélémy, Clerc) is an interesting case of a small but tight and independent co-editor network.\nIn addition, many of these sub-networks are loosely connected between each other, while others are entirely disconnected from the main network. And some co-editors serve as connections between several sub-networks. This depends the most, however, at least in my experience, on the number of editors considered and would be best analyzed statistically on the entire network by way of determining nodes with a high betweenness centrality (rather than degree centrality)."
  },
  {
    "objectID": "coeditors.html#an-additional-look-at-the-graph-data",
    "href": "coeditors.html#an-additional-look-at-the-graph-data",
    "title": "Co-editors",
    "section": "An additional look at the graph data",
    "text": "An additional look at the graph data\nBased on the data modeled as a graph, we can not only visualize it, but also extract some additional information. This is done here for the entire dataset, not just the top coeditors as in the visualization.\nThis essentially shows that, while some editor pairs have worked together particularly frequently (think Herman and Pelckman or Hasquin and Mortier above), they are not necessarily the ones who have engaged in the most co-editorships overall. Lüsebrink and Porret are clearly the people who have engaged in the most co-editor relationships (i.e., cumulated number of co-editors involved in all volumes they co-edited).\nWe can also check, as mentioned above, for betweenness centrality of the editors, in order to identify those editors (in the entire graph) that tend to connect otherwise weakly-connected sub-networks.\n\n\nCode\ndef calculate_measures(ccc): \n\n    # Prepare the dataset\n    ccc.rename(columns={\"count\": \"weight\"}, inplace=True)\n    ccc = ccc[ccc[\"weight\"] &gt; 0] # for filtering (0 is slow but complete)\n\n    # Load the data into a NetworkX graph\n    G = nx.Graph()\n    for line in ccc.iterrows(): \n        G.add_edge(\n            line[1][0],\n            line[1][1],\n            weight=line[1][2],\n            )\n    print(\"Number of editors included in this Graph: \" + str(nx.number_of_nodes(G)) + \".\")\n\n    # Get the degrees\n    dg = dict(G.degree)\n\n    # Get the betweenness centralities\n    # See: https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.centrality.betweenness_centrality.html\n    bc = nx.betweenness_centrality(\n        G,\n        k=None,\n        normalized=True,\n        weight=None,\n        endpoints=False,\n        seed=None\n        )\n\n    # Get eigenvector centrality\n    ec = nx.eigenvector_centrality(\n        G,\n        max_iter=100,\n        tol=1e-03\n        )\n    \n    # Merge results into a dataframe\n    dg = pd.DataFrame.from_dict(dg, orient=\"index\", columns=[\"degree\"])\n    dg[\"coeditor\"] = dg.index\n    bc = pd.DataFrame.from_dict(bc, orient=\"index\", columns=[\"betweenness\"])\n    bc[\"coeditor\"] = bc.index\n    ec = pd.DataFrame.from_dict(ec, orient=\"index\", columns=[\"eigenvector\"])\n    ec[\"coeditor\"] = ec.index\n    #measures = dg.merge(ec, how=\"inner\", on=\"coeditor\")\n    measures = dg.merge(bc, on='coeditor').merge(ec,on='coeditor')\n    measures = measures[[\"coeditor\", \"degree\", \"betweenness\", \"eigenvector\"]]\n\n    return measures\nmeasures = calculate_measures(ccc)\n\n\nNumber of editors included in this Graph: 5857.\n\n\nNow we can look at the various graph measures calculated above on the entire network of editors.\nFirst, the editors with the highes value for degree (number of connections). This describes the total number of co-editorships a given person has engaged in.\n\n\nCode\ndef show_degree(measures): \n    measures.sort_values(by=\"degree\", ascending=False, inplace=True)\n    degree = measures.drop([\"betweenness\", \"eigenvector\"], axis=1, inplace=False)\n    display(HTML(degree.head(15).to_html(index=False)))\nshow_degree(measures)\n\n\n\n\n\ncoeditor\ndegree\n\n\n\n\nLüsebrink_Hans-Jürgen\n23\n\n\nPorret_Michel\n22\n\n\nDelon_Michel\n18\n\n\nBourdin_Philippe\n18\n\n\nBerchtold_Jacques\n18\n\n\nSeth_Catriona\n18\n\n\nPaganini_Gianni\n17\n\n\nLeuwers_Hervé\n17\n\n\nBertrand_Gilles\n15\n\n\nMoureau_François\n15\n\n\nSermain_Jean-Paul\n14\n\n\nBiard_Michel\n14\n\n\nLotterie_Florence\n14\n\n\nHébert_Pierre\n13\n\n\nHerman_Jan\n13\n\n\n\n\n\nSimilarly, we can look at the betweenness centrality. This is a measure that describes how much an editor acts as a connection between various subnetworks. “Betweenness centrality quantifies the number of times a node acts as a bridge along the shortest path between two other nodes”, as per Wikipedia.\n\n\nCode\ndef show_betweenness(measures): \n    measures.sort_values(by=\"betweenness\", ascending=False, inplace=True)\n    betweenness = measures.drop([\"eigenvector\"], axis=1, inplace=False)\n    display(HTML(betweenness.head(15).to_html(index=False)))\nshow_betweenness(measures)\n\n\n\n\n\ncoeditor\ndegree\nbetweenness\n\n\n\n\nPorret_Michel\n22\n0.010365\n\n\nBerchtold_Jacques\n18\n0.009650\n\n\nSeth_Catriona\n18\n0.009246\n\n\nBernier_Marc André\n11\n0.008750\n\n\nDelon_Michel\n18\n0.006522\n\n\nMoureau_François\n15\n0.005221\n\n\nVolpilhac-Auger_Catherine\n11\n0.004951\n\n\nLotterie_Florence\n14\n0.004941\n\n\nHersant_Marc\n11\n0.004284\n\n\nPlagnol-Diéval_Marie-Emmanuelle\n11\n0.004219\n\n\nMondot_Jean\n10\n0.004156\n\n\nKrief_Huguette\n8\n0.004055\n\n\nVasak_Anouchka\n5\n0.004007\n\n\nLüsebrink_Hans-Jürgen\n23\n0.003794\n\n\nPoirson_Martial\n11\n0.003758\n\n\n\n\n\nFinally, we can look at eigenvector centrality, a measure of influence in a network.\n\n\nCode\ndef show_eigenvector(measures): \n    measures.sort_values(by=\"eigenvector\", ascending=False, inplace=True)\n    eigenvector = measures.drop([\"betweenness\"], axis=1, inplace=False)\n    display(HTML(eigenvector.head(15).to_html(index=False)))\nshow_eigenvector(measures)\n\n\n\n\n\ncoeditor\ndegree\neigenvector\n\n\n\n\nHébert_Pierre\n13\n0.257840\n\n\nLuneau_Marie-Pier\n12\n0.252006\n\n\nMichon_Jacques\n10\n0.250391\n\n\nVincent_Josée\n9\n0.243162\n\n\nMylène Fréchette_Fanie St-Laurent\n9\n0.243162\n\n\nGodbout_Patricia\n9\n0.243162\n\n\nDrouin_de Sophie\n9\n0.243162\n\n\nBrisson_Frédéric\n9\n0.243162\n\n\nPouliot_Suzanne\n9\n0.243162\n\n\nMarcel Lajeunesse_Éric Leroux\n9\n0.243162\n\n\nBiard_Michel\n14\n0.153647\n\n\nLeuwers_Hervé\n17\n0.146392\n\n\nBourdin_Philippe\n18\n0.144208\n\n\nPorret_Michel\n22\n0.119891\n\n\nSerna_Pierre\n10\n0.116381\n\n\n\n\n\nThe eigenvector centrality is interesting because it surfaces editors who do not necessarily have very high degree or betweenness centrality scores, like Pierre Hébert. It remains to be seen what exactly this says about them!"
  },
  {
    "objectID": "languages.html",
    "href": "languages.html",
    "title": "Languages",
    "section": "",
    "text": "This section provides some analyses of the languages of publications referenced in the Bibliographie.\nCode\n# === Imports === \n\nimport re \nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom os.path import join\nfrom os.path import realpath, dirname\nimport os\nfrom lxml import etree\nfrom io import StringIO, BytesIO\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\n\n\n# === Files and parameters === \n\nwdir = join(\"/\", \"media\", \"christof\", \"Data\", \"Github\", \"christofs\", \"BIB18\")\nbibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF.rdf\") \n#bibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF_TEST.rdf\") \n\nnamespaces = {\n    \"foaf\" : \"http://xmlns.com/foaf/0.1/\",\n    \"bib\" : \"http://purl.org/net/biblio#\",\n    \"dc\" : \"http://purl.org/dc/elements/1.1/\",\n    \"z\" : \"http://www.zotero.org/namespaces/export#\",\n    \"rdf\" : \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n    }\n\n# === Load the dataset === \n\ndef read_xml(bibdatafile): \n    bibdata = etree.parse(bibdatafile)\n    return bibdata\nbibdata = read_xml(bibdatafile)"
  },
  {
    "objectID": "languages.html#first-approach-language",
    "href": "languages.html#first-approach-language",
    "title": "Languages",
    "section": "First approach: ‘language’",
    "text": "First approach: ‘language’\nWe can use the “language” field in the dataset. This is precise, technically speaking, but depends on the level of curation of the dataset. The language field is not fully reliable at this time.\nWe don’t have abstracts or full texts in the dataset, but on the basis of the titles alone, the (most likely) language of the publication has been determined. Note that an algorithmic process, based on the library py-lingua and using only the sometimes very short titles, has been used to create this data, so errors are to be expected. With the progress of corrections in the dataset, this will improve over time.\n\n\nCode\ndef get_languages(bibdata): \n    print(\"\\nLanguages\")\n\n    # Find all the instances of \"language\" Element and its content\n    xpath = \"//z:language/text()\"\n    languages = bibdata.xpath(xpath, namespaces=namespaces)\n    # Identify frequency of languages\n    languages_counts = Counter(languages)\n    languages_counts = dict(sorted(languages_counts.items(), key = lambda item: item[1], reverse=True)[:10])\n    \n \n    # Visualize using a simple bar chart \n    lc = pd.DataFrame.from_dict(languages_counts, orient=\"index\", columns=[\"count\"]).reset_index().rename({\"index\" : \"language\"}, axis=1)\n    plt.figure(figsize=(12,6))\n    pal = sns.color_palette(\"colorblind\", len(lc))\n    fig = sns.barplot(data=lc, x=\"language\", y=\"count\", palette=pal)\n    fig.set_xticklabels(fig.get_xticklabels(), rotation=30)\n    for i in fig.containers:\n        fig.bar_label(i,)\n    plt.tight_layout()\n    plt.savefig(join(wdir, \"figures\", \"languages_counts.png\"), dpi=300)\n    return languages, languages_counts\n  \nlanguages, languages_counts = get_languages(bibdata)\n\n\n\n\nLanguages\n\n\n\n\n\n\n\nCode\n    \n# Provide some results as a text. \nprint(\"There are \" + str(len(languages))  + \" instances of language in the dataset.\")\nprint(\"At the moment, only \" + str(len(languages_counts)) + \" different languages are considered for analysis.\")\nlanguages_perc = {k: v / len(languages) for k, v in languages_counts.items()}\nprint(\"The most prevalent language is \" + str(list(languages_perc.keys())[0]) + \", with \" + \"{:2.2%}\".format(list(languages_perc.values())[0]) + \" of all entries.\")\n\n\n\n\nThere are 64396 instances of language in the dataset.\nAt the moment, only 8 different languages are considered for analysis.\nThe most prevalent language is French, with 73.89% of all entries."
  },
  {
    "objectID": "people.html",
    "href": "people.html",
    "title": "People",
    "section": "",
    "text": "This page provides information about person names, i.e. authors and editors.\n\n\nCode\n# === Imports === \n\nimport re \nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom os.path import join\nfrom os.path import realpath, dirname\nimport os\nfrom lxml import etree\nfrom io import StringIO, BytesIO\nfrom collections import Counter\nimport pandas as pd\n\n\n# === Files and parameters === \n\nwdir = join(\"/\", \"media\", \"christof\", \"Data\", \"Github\", \"christofs\", \"BIB18\")\nbibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF.rdf\") \n#bibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF_TEST.rdf\") \n\n\nnamespaces = {\n    \"foaf\" : \"http://xmlns.com/foaf/0.1/\",\n    \"bib\" : \"http://purl.org/net/biblio#\",\n    \"dc\" : \"http://purl.org/dc/elements/1.1/\",\n    \"z\" : \"http://www.zotero.org/namespaces/export#\",\n    \"rdf\" : \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n    }\n\n\ndef read_json(bibdatafile): \n    bibdata = etree.parse(bibdatafile)\n    return bibdata\n\nbibdata = read_json(bibdatafile)\n\n\n\n\nCode\ndef get_personnames(bibdata): \n\n    # Find all the instances of persons\n    personnames = []\n    xpath = \"//foaf:Person\"\n    persons = bibdata.xpath(xpath, namespaces=namespaces)\n    print(\"There are \" + str(len(persons)) + \" instances of the Element 'person' in the dataset.\")\n\n    # Get the names (full name or first name, last name) from each person\n    for item in persons: \n        if len(item) == 1: \n            personname = item[0].text\n            personnames.append(personname)\n        elif len(item) == 2: \n            personname = item[0].text + \", \" + item[1].text \n            personnames.append(personname)    \n    print(\"There are \" + str(len(Counter(personnames))) + \" different person names in the dataset.\")\n    return personnames\n\nglobal personnames\npersonnames  = get_personnames(bibdata)\n\n\nThere are 94321 instances of the Element 'person' in the dataset.\nThere are 31026 different person names in the dataset."
  },
  {
    "objectID": "people.html#most-frequently-occurring-person-names",
    "href": "people.html#most-frequently-occurring-person-names",
    "title": "People",
    "section": "",
    "text": "This page provides information about person names, i.e. authors and editors.\n\n\nCode\n# === Imports === \n\nimport re \nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom os.path import join\nfrom os.path import realpath, dirname\nimport os\nfrom lxml import etree\nfrom io import StringIO, BytesIO\nfrom collections import Counter\nimport pandas as pd\n\n\n# === Files and parameters === \n\nwdir = join(\"/\", \"media\", \"christof\", \"Data\", \"Github\", \"christofs\", \"BIB18\")\nbibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF.rdf\") \n#bibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF_TEST.rdf\") \n\n\nnamespaces = {\n    \"foaf\" : \"http://xmlns.com/foaf/0.1/\",\n    \"bib\" : \"http://purl.org/net/biblio#\",\n    \"dc\" : \"http://purl.org/dc/elements/1.1/\",\n    \"z\" : \"http://www.zotero.org/namespaces/export#\",\n    \"rdf\" : \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n    }\n\n\ndef read_json(bibdatafile): \n    bibdata = etree.parse(bibdatafile)\n    return bibdata\n\nbibdata = read_json(bibdatafile)\n\n\n\n\nCode\ndef get_personnames(bibdata): \n\n    # Find all the instances of persons\n    personnames = []\n    xpath = \"//foaf:Person\"\n    persons = bibdata.xpath(xpath, namespaces=namespaces)\n    print(\"There are \" + str(len(persons)) + \" instances of the Element 'person' in the dataset.\")\n\n    # Get the names (full name or first name, last name) from each person\n    for item in persons: \n        if len(item) == 1: \n            personname = item[0].text\n            personnames.append(personname)\n        elif len(item) == 2: \n            personname = item[0].text + \", \" + item[1].text \n            personnames.append(personname)    \n    print(\"There are \" + str(len(Counter(personnames))) + \" different person names in the dataset.\")\n    return personnames\n\nglobal personnames\npersonnames  = get_personnames(bibdata)\n\n\nThere are 94321 instances of the Element 'person' in the dataset.\nThere are 31026 different person names in the dataset."
  },
  {
    "objectID": "people.html#visualization-of-the-most-frequent-person-names",
    "href": "people.html#visualization-of-the-most-frequent-person-names",
    "title": "People",
    "section": "Visualization of the most frequent person names",
    "text": "Visualization of the most frequent person names\nThese persons could be authors or editors of publications.\n\n\nCode\ndef most_frequent_persons(personnames):\n    # Count the occurrences, find the 10 most frequently mentioned publishers\n    personnames_counts = Counter(personnames)\n    personnames_counts = dict(\n        sorted(personnames_counts.items(), \n        key = lambda item: item[1], reverse=True)[:20]\n        )\n    \n    columns = [\"count\"]\n    personnames_counts = pd.DataFrame.from_dict(\n        personnames_counts, \n        orient=\"index\", \n        columns=[\"count\"]).reset_index().rename({\"index\" : \"person\"}, \n        axis=1\n        )\n    \n    #print(personnames_counts)\n    return personnames_counts\n\n\ndef visualize_personnames_counts(personnames_counts): \n    #print(publishernames_counts)\n    plt.figure(figsize=(12,8))\n    pal = sns.color_palette(\"colorblind\", len(personnames_counts))\n    fig = sns.barplot(\n        data=personnames_counts, \n        y=\"person\", \n        x=\"count\", \n        orient='h' \n        )\n    for i in fig.containers:\n        fig.bar_label(i,)\n    plt.tight_layout()\n    plt.savefig(\n        join(wdir, \"figures\", \"personnames_counts.png\"),\n        dpi=300\n        )\n\n\npersonnames_counts = most_frequent_persons(personnames)\nvisualize_personnames_counts(personnames_counts)\n\n\nThere are 31026 different person names in the dataset."
  },
  {
    "objectID": "publishers.html",
    "href": "publishers.html",
    "title": "Publishers",
    "section": "",
    "text": "This page provides information about the publishers mentioned in the dataset.\nCode\n# === Imports === \n\nimport re \nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom os.path import join\nfrom os.path import realpath, dirname\nimport os\nfrom lxml import etree\nfrom io import StringIO, BytesIO\nfrom collections import Counter\nimport pandas as pd\n\n\n# === Files and parameters === \n\nwdir = join(\"/\", \"media\", \"christof\", \"Data\", \"Github\", \"christofs\", \"BIB18\")\nbibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF.rdf\") \n#bibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF_TEST.rdf\") \n\n\nnamespaces = {\n    \"foaf\" : \"http://xmlns.com/foaf/0.1/\",\n    \"bib\" : \"http://purl.org/net/biblio#\",\n    \"dc\" : \"http://purl.org/dc/elements/1.1/\",\n    \"z\" : \"http://www.zotero.org/namespaces/export#\",\n    \"rdf\" : \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n    }\n\n\ndef read_json(bibdatafile): \n    bibdata = etree.parse(bibdatafile)\n    return bibdata\n\nbibdata = read_json(bibdatafile)\nCode\ndef get_publishers(bibdata): \n    # Find all the instances of publisher names\n    publishers = []\n    xpath = \"//dc:publisher//foaf:name/text()\"\n    publishers = bibdata.xpath(xpath, namespaces=namespaces)    \n    # Show some results\n    print(\"There are \" + str(len(publishers)) + \" instances of publishers mentioned in the dataset.\" )\n    print(\"There are a total of \" + str(len(set(publishers))) + \" different publishers mentioned in the dataset.\" )\n    return publishers\n\n\ndef most_frequent_publishers(publishers):\n    # Count the occurrences, find the 10 most frequently mentioned publishers\n    publishernames_counts = Counter(publishers)\n    publishernames_counts = dict(\n        sorted(publishernames_counts.items(), \n        key = lambda item: item[1], reverse=True)[:20]\n        )\n    \n    columns = [\"occurrences\"]\n    publishernames_counts = pd.DataFrame.from_dict(\n        publishernames_counts, \n        orient=\"index\", \n        columns=[\"count\"]).reset_index().rename({\"index\" : \"publisher\"}, \n        axis=1\n        )\n    return publishernames_counts\n\n\nglobal publishernames_counts\npublishers = get_publishers(bibdata)\npublishernames_counts = most_frequent_publishers(publishers)\n\n\nThere are 37781 instances of publishers mentioned in the dataset.\nThere are a total of 5705 different publishers mentioned in the dataset."
  },
  {
    "objectID": "publishers.html#visualization",
    "href": "publishers.html#visualization",
    "title": "Publishers",
    "section": "Visualization",
    "text": "Visualization\n\n\nCode\n\ndef visualize_publishername_counts(publishernames_counts): \n    #print(publishernames_counts)\n    plt.figure(figsize=(12,8))\n    pal = sns.color_palette(\"colorblind\", len(publishernames_counts))\n    fig = sns.barplot(\n        data=publishernames_counts, \n        y=\"publisher\", \n        x=\"count\", \n        orient='h' \n        )\n    for i in fig.containers:\n        fig.bar_label(i,)\n    plt.tight_layout()\n    plt.savefig(\n        join(wdir, \"figures\", \"publishernames_counts.png\"),\n        dpi=300\n        )\n    \nvisualize_publishername_counts(publishernames_counts)"
  },
  {
    "objectID": "time.html",
    "href": "time.html",
    "title": "Time",
    "section": "",
    "text": "Here, only data starting in 1986 is shown (the bibliography was launched in 1992).\n\n\nCode\n# === Imports === \n\nimport re \nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom os.path import join\nfrom os.path import realpath, dirname\nimport os\nfrom lxml import etree\nfrom io import StringIO, BytesIO\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\n\n\n# === Files and parameters === \n\nwdir = join(\"/\", \"media\", \"christof\", \"Data\", \"Github\", \"christofs\", \"BIB18\")\nbibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF.rdf\") \n#bibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF_TEST.rdf\") \n\nnamespaces = {\n    \"foaf\" : \"http://xmlns.com/foaf/0.1/\",\n    \"bib\" : \"http://purl.org/net/biblio#\",\n    \"dc\" : \"http://purl.org/dc/elements/1.1/\",\n    \"z\" : \"http://www.zotero.org/namespaces/export#\",\n    \"rdf\" : \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n    }\n\n# === Load the dataset === \n\ndef read_xml(bibdatafile): \n    bibdata = etree.parse(bibdatafile)\n    return bibdata\nbibdata = read_xml(bibdatafile)\n\n\n\n\nCode\n\ndef get_pubyears(bibdata): \n    # Setting things up\n    pubyears = []\n\n    # Find all the instances of publication dates\n    xpath = \"//dc:date/text()\"\n    pubyears = bibdata.xpath(xpath, namespaces=namespaces)\n\n    # Count the occurrences, find the 10 most frequently mentioned publication dates\n    pubyear_counts = Counter(pubyears)\n    pubyear_counts = dict(sorted(pubyear_counts.items(), reverse=False))\n\n    # Filter data to clean it\n    pubyear_counts = pd.DataFrame.from_dict(pubyear_counts, orient=\"index\").reset_index().rename(mapper={\"index\":\"year\", 0 : \"count\"}, axis=\"columns\")    #pubyear_counts = pubyear_counts[pubyear_counts[0] == 1991]\n    pubyear_counts = pubyear_counts[pubyear_counts[\"year\"].str.isnumeric()]\n    pubyear_counts.set_index(\"year\", inplace=True)\n    # Remove erroneous years (to be corrected in the data)\n    pubyear_counts.drop([\"134\", \"207\", \"22\", \"30\", \"42\", \"58\", \"76\", \"78\", \"20\", \"201\"], inplace=True)\n    # Remove less relevant years (optional, of course)\n    pubyear_counts.drop([\"1815\", \"1834\", \"1891\", \"1932\", \"1945\", \"1957\", \"1961\", \"1969\", \"1973\", \"1974\", \"1975\", \"1978\", \"1979\", \"1981\", \"1982\", \"1983\", \"1984\"], inplace=True)\n    pubyear_counts.reset_index(inplace=True)\n    #print(pubyear_counts.head())\n\n    # Display some key figures\n    print(\"There are \" + str(len(pubyears)) + \" instances of publication years mentioned in the dataset.\")\n    print(\"There are \" + str(len(pubyear_counts)) + \" different years mentioned in the dataset.\")\n\n    return pubyear_counts\n\n\ndef visualize_pubyears(pubyear_counts):\n    plt.figure(figsize=(12,8))\n    pal = sns.color_palette(\"colorblind\", len(pubyear_counts))\n    fig = sns.barplot(\n        data = pubyear_counts, \n        x=\"year\", \n        y=\"count\",\n        palette = pal,\n        )\n    fig.set_xticklabels(fig.get_xticklabels(), rotation=45)\n    plt.savefig(join(wdir, \"figures\", \"pubyear_counts.png\"), dpi=300)\n\n\npubyear_counts = get_pubyears(bibdata)\nvisualize_pubyears(pubyear_counts)\n\n\nThere are 64169 instances of publication years mentioned in the dataset.\nThere are 38 different years mentioned in the dataset."
  },
  {
    "objectID": "time.html#distribution-of-publications-per-year",
    "href": "time.html#distribution-of-publications-per-year",
    "title": "Time",
    "section": "",
    "text": "Here, only data starting in 1986 is shown (the bibliography was launched in 1992).\n\n\nCode\n# === Imports === \n\nimport re \nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom os.path import join\nfrom os.path import realpath, dirname\nimport os\nfrom lxml import etree\nfrom io import StringIO, BytesIO\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\n\n\n# === Files and parameters === \n\nwdir = join(\"/\", \"media\", \"christof\", \"Data\", \"Github\", \"christofs\", \"BIB18\")\nbibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF.rdf\") \n#bibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF_TEST.rdf\") \n\nnamespaces = {\n    \"foaf\" : \"http://xmlns.com/foaf/0.1/\",\n    \"bib\" : \"http://purl.org/net/biblio#\",\n    \"dc\" : \"http://purl.org/dc/elements/1.1/\",\n    \"z\" : \"http://www.zotero.org/namespaces/export#\",\n    \"rdf\" : \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n    }\n\n# === Load the dataset === \n\ndef read_xml(bibdatafile): \n    bibdata = etree.parse(bibdatafile)\n    return bibdata\nbibdata = read_xml(bibdatafile)\n\n\n\n\nCode\n\ndef get_pubyears(bibdata): \n    # Setting things up\n    pubyears = []\n\n    # Find all the instances of publication dates\n    xpath = \"//dc:date/text()\"\n    pubyears = bibdata.xpath(xpath, namespaces=namespaces)\n\n    # Count the occurrences, find the 10 most frequently mentioned publication dates\n    pubyear_counts = Counter(pubyears)\n    pubyear_counts = dict(sorted(pubyear_counts.items(), reverse=False))\n\n    # Filter data to clean it\n    pubyear_counts = pd.DataFrame.from_dict(pubyear_counts, orient=\"index\").reset_index().rename(mapper={\"index\":\"year\", 0 : \"count\"}, axis=\"columns\")    #pubyear_counts = pubyear_counts[pubyear_counts[0] == 1991]\n    pubyear_counts = pubyear_counts[pubyear_counts[\"year\"].str.isnumeric()]\n    pubyear_counts.set_index(\"year\", inplace=True)\n    # Remove erroneous years (to be corrected in the data)\n    pubyear_counts.drop([\"134\", \"207\", \"22\", \"30\", \"42\", \"58\", \"76\", \"78\", \"20\", \"201\"], inplace=True)\n    # Remove less relevant years (optional, of course)\n    pubyear_counts.drop([\"1815\", \"1834\", \"1891\", \"1932\", \"1945\", \"1957\", \"1961\", \"1969\", \"1973\", \"1974\", \"1975\", \"1978\", \"1979\", \"1981\", \"1982\", \"1983\", \"1984\"], inplace=True)\n    pubyear_counts.reset_index(inplace=True)\n    #print(pubyear_counts.head())\n\n    # Display some key figures\n    print(\"There are \" + str(len(pubyears)) + \" instances of publication years mentioned in the dataset.\")\n    print(\"There are \" + str(len(pubyear_counts)) + \" different years mentioned in the dataset.\")\n\n    return pubyear_counts\n\n\ndef visualize_pubyears(pubyear_counts):\n    plt.figure(figsize=(12,8))\n    pal = sns.color_palette(\"colorblind\", len(pubyear_counts))\n    fig = sns.barplot(\n        data = pubyear_counts, \n        x=\"year\", \n        y=\"count\",\n        palette = pal,\n        )\n    fig.set_xticklabels(fig.get_xticklabels(), rotation=45)\n    plt.savefig(join(wdir, \"figures\", \"pubyear_counts.png\"), dpi=300)\n\n\npubyear_counts = get_pubyears(bibdata)\nvisualize_pubyears(pubyear_counts)\n\n\nThere are 64169 instances of publication years mentioned in the dataset.\nThere are 38 different years mentioned in the dataset."
  },
  {
    "objectID": "titles.html",
    "href": "titles.html",
    "title": "Titles",
    "section": "",
    "text": "The titles included in the bibliography tell us a few things, for instance the language a publication is written in or the main themes (keywords, authors) that is the subject of a publication.\nCode\n# === Imports === \n\nimport re \nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom os.path import join\nfrom os.path import realpath, dirname\nimport os\nfrom lxml import etree\nfrom io import StringIO, BytesIO\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\n\n\n# === Files and parameters === \n\nwdir = join(\"/\", \"media\", \"christof\", \"Data\", \"Github\", \"christofs\", \"BIB18\")\nbibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF.rdf\") \n#bibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF_TEST.rdf\") \nstopwords_fra_file = join(wdir, \"results\", \"stopwords-fra.txt\")\nstopwords_eng_file = join(wdir, \"results\", \"stopwords-eng.txt\")\n\n\nnamespaces = {\n    \"foaf\" : \"http://xmlns.com/foaf/0.1/\",\n    \"bib\" : \"http://purl.org/net/biblio#\",\n    \"dc\" : \"http://purl.org/dc/elements/1.1/\",\n    \"z\" : \"http://www.zotero.org/namespaces/export#\",\n    \"rdf\" : \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n    }\n\n\n# === Load the dataset === \n\ndef read_xml(bibdatafile): \n    bibdata = etree.parse(bibdatafile)\n    return bibdata\nbibdata = read_xml(bibdatafile)"
  },
  {
    "objectID": "titles.html#extracting-the-titles",
    "href": "titles.html#extracting-the-titles",
    "title": "Titles",
    "section": "Extracting the titles",
    "text": "Extracting the titles\nThe first step is to identify the titles in the dataset. At the moment, the primary titles of journal articles, book chapters and books are taken into account.\n\n\nCode\ndef get_titles(bibdata): \n    \"\"\"\n    Extract all the primary titles from the dataset. \n    Primary titles are all titles except journal names. \n    \"\"\"\n    # Find all primary \"title\" elements in the dataset \n    titles = []\n\n    # Article titles\n    xpath = \"//bib:Article/dc:title/text()\"\n    article_titles = bibdata.xpath(xpath, namespaces=namespaces)\n    titles.extend(article_titles)\n\n    # Book titles\n    xpath = \"//bib:Book/dc:title/text()\"\n    book_titles = bibdata.xpath(xpath, namespaces=namespaces)\n    titles.extend(book_titles)\n\n    # Book chapter titles\n    xpath = \"//bib:BookSection/dc:title/text()\"\n    bookchapter_titles = bibdata.xpath(xpath, namespaces=namespaces)\n    titles.extend(bookchapter_titles)\n\n    print(\"Number of titles found: \" + str(len(titles)) + \".\")\n    return titles\n\n# === Main === \n\nglobal titles \ntitles = get_titles(bibdata)\n\n\nNumber of titles found: 62987.\n\n\nThe following step joins all words in all titles, then filters the list using French and English stopwords.\n\n\nCode\ndef load_stopwords(stopwordsfile): \n    with open(stopwordsfile, \"r\", encoding=\"utf8\") as infile: \n        stopwords = infile.read().split(\"\\n\")\n    #print(stopwords)\n    return stopwords \n\n    \ndef get_keywords(titles, stopwords_fra, stopwords_eng): \n    \"\"\"\n    Identify recurring, content-bearing words in the titles. \n    Returns: dict (keyword : frequency)\n    \"\"\"\n    \n    # Filtering of words in all titles\n    titles = \" \".join(titles)\n    titlewords = re.split(\"\\W+\", titles)\n    titlewords = [word.lower() for word in titlewords if word.lower() not in stopwords_fra]\n    titlewords = [word.lower() for word in titlewords if word.lower() not in stopwords_eng]\n    #print(titles[0:20])\n    \n    # Establish counts, transform to DataFrame, sort to select n most frequent words\n    titlewords_counts = dict(Counter(titlewords))\n    titlewords_counts = pd.DataFrame.from_dict(\n        titlewords_counts, \n        orient=\"index\", \n        columns=[\"count\"]).reset_index().rename({\"index\" : \"word\"}, \n        )      \n    titlewords_counts.sort_values(by=\"count\", ascending=False, inplace=True)\n    titlewords_counts = titlewords_counts[0:50]\n   \n    #print(titlewords_counts.head())\n    return titlewords_counts\n\n\nglobal titlewords_counts\nstopwords_fra = load_stopwords(stopwords_fra_file)\nstopwords_eng = load_stopwords(stopwords_eng_file)\ntitlewords_counts = get_keywords(titles, stopwords_fra, stopwords_eng)\n\n\nThe following step provides a visualization of the most frequently used words in the titles.\n\n\nCode\ndef visualize_titlewords(titlewords_counts): \n    plt.figure(figsize=(12,12))\n    pal = sns.color_palette(\"colorblind\", len(titlewords_counts))\n    fig = sns.barplot(\n        data=titlewords_counts, \n        y=\"index\", \n        x=\"count\", \n        orient='h' \n        )\n    for i in fig.containers:\n        fig.bar_label(i,)\n    plt.tight_layout()\n    plt.savefig(\n        join(wdir, \"figures\", \"titlewords_counts.png\"),\n        dpi=600\n        )\n    \nvisualize_titlewords(titlewords_counts)\n\n\n\n\n\nAs can be seen from this table, there are a few authors who appear with particularly high frequency in the titles: Rousseau, Voltaire, Diderot (in descending order).\nIn terms of thematic keywords, apart from indicators of the time period and geographic focus, the following terms stand out (again, in descending order of frequency): histoire, Lumières, révolution, lettres, history, Europe, Enlightenment, art, théâtre, revolution, roman, littérature, correspondance, voyage, culture, etc."
  },
  {
    "objectID": "types.html",
    "href": "types.html",
    "title": "Types",
    "section": "",
    "text": "There are 64397 instances of publication type (each entry has one), with just 6 different types of publication types. This number results from the original bibliography’s data model, but the labels used below are Zotero’s labels. The exception to this rule is that the distinction between monographs and edited volumes is lost in Zotero, which considers both to be books.\n\njournalArticle: 26615\nbook (monographs and edited volumes): 23083\nbookSection: 13368\nthesis: 1296\ndataset: 34\nwebpage: 1\n\n(Visualize as a bubble chart?)\n\n\nCode\n# === Imports === \n\nimport re \nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom os.path import join\nfrom os.path import realpath, dirname\nimport os\nfrom lxml import etree\nfrom io import StringIO, BytesIO\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\n\n\n# === Files and parameters === \n\nwdir = join(\"/\", \"media\", \"christof\", \"Data\", \"Github\", \"christofs\", \"BIB18\")\nbibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF.rdf\") \n#bibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF_TEST.rdf\") \n\nnamespaces = {\n    \"foaf\" : \"http://xmlns.com/foaf/0.1/\",\n    \"bib\" : \"http://purl.org/net/biblio#\",\n    \"dc\" : \"http://purl.org/dc/elements/1.1/\",\n    \"z\" : \"http://www.zotero.org/namespaces/export#\",\n    \"rdf\" : \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n    }\n\n# === Load the dataset === \n\ndef read_xml(bibdatafile): \n    bibdata = etree.parse(bibdatafile)\n    return bibdata\nbibdata = read_xml(bibdatafile)\n\n\n\n\nCode\ndef get_pubtypes(bibdata): \n    # Find all the instances of publication types\n    pubtypes = []\n    xpath = \"//z:itemType/text()\"\n    pubtypes = bibdata.xpath(xpath, namespaces=namespaces)\n    print(\"There are \" + str(len(pubtypes)) +  \" instances of publication type in the dataset.\")\n    return pubtypes\n\n\ndef most_frequent_pubtypes(pubtypes): \n    # Count the occurrences, find the 10 most frequently mentioned persons\n    pubtypes_counts = Counter(pubtypes)\n    print(\"There are \" + str(len(pubtypes_counts)) + \" different publication types in the dataset.\")\n    pubtypes_counts = dict(sorted(pubtypes_counts.items(), key = lambda item: item[1], reverse=True)[:10])\n    columns = [\"counts\"]\n    pubtypes_counts = pd.DataFrame.from_dict(\n        pubtypes_counts, \n        orient=\"index\", \n        columns=[\"count\"]).reset_index().rename({\"index\" : \"types\"}, \n        axis=1\n        )\n    return pubtypes_counts\n\n\nglobal pubtypes_counts\npubtypes = get_pubtypes(bibdata)\npubtypes_counts = most_frequent_pubtypes(pubtypes)\n\n\nThere are 64397 instances of publication type in the dataset.\nThere are 6 different publication types in the dataset."
  },
  {
    "objectID": "types.html#distribution-of-publication-types-in-the-dataset",
    "href": "types.html#distribution-of-publication-types-in-the-dataset",
    "title": "Types",
    "section": "",
    "text": "There are 64397 instances of publication type (each entry has one), with just 6 different types of publication types. This number results from the original bibliography’s data model, but the labels used below are Zotero’s labels. The exception to this rule is that the distinction between monographs and edited volumes is lost in Zotero, which considers both to be books.\n\njournalArticle: 26615\nbook (monographs and edited volumes): 23083\nbookSection: 13368\nthesis: 1296\ndataset: 34\nwebpage: 1\n\n(Visualize as a bubble chart?)\n\n\nCode\n# === Imports === \n\nimport re \nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom os.path import join\nfrom os.path import realpath, dirname\nimport os\nfrom lxml import etree\nfrom io import StringIO, BytesIO\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\n\n\n# === Files and parameters === \n\nwdir = join(\"/\", \"media\", \"christof\", \"Data\", \"Github\", \"christofs\", \"BIB18\")\nbibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF.rdf\") \n#bibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF_TEST.rdf\") \n\nnamespaces = {\n    \"foaf\" : \"http://xmlns.com/foaf/0.1/\",\n    \"bib\" : \"http://purl.org/net/biblio#\",\n    \"dc\" : \"http://purl.org/dc/elements/1.1/\",\n    \"z\" : \"http://www.zotero.org/namespaces/export#\",\n    \"rdf\" : \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n    }\n\n# === Load the dataset === \n\ndef read_xml(bibdatafile): \n    bibdata = etree.parse(bibdatafile)\n    return bibdata\nbibdata = read_xml(bibdatafile)\n\n\n\n\nCode\ndef get_pubtypes(bibdata): \n    # Find all the instances of publication types\n    pubtypes = []\n    xpath = \"//z:itemType/text()\"\n    pubtypes = bibdata.xpath(xpath, namespaces=namespaces)\n    print(\"There are \" + str(len(pubtypes)) +  \" instances of publication type in the dataset.\")\n    return pubtypes\n\n\ndef most_frequent_pubtypes(pubtypes): \n    # Count the occurrences, find the 10 most frequently mentioned persons\n    pubtypes_counts = Counter(pubtypes)\n    print(\"There are \" + str(len(pubtypes_counts)) + \" different publication types in the dataset.\")\n    pubtypes_counts = dict(sorted(pubtypes_counts.items(), key = lambda item: item[1], reverse=True)[:10])\n    columns = [\"counts\"]\n    pubtypes_counts = pd.DataFrame.from_dict(\n        pubtypes_counts, \n        orient=\"index\", \n        columns=[\"count\"]).reset_index().rename({\"index\" : \"types\"}, \n        axis=1\n        )\n    return pubtypes_counts\n\n\nglobal pubtypes_counts\npubtypes = get_pubtypes(bibdata)\npubtypes_counts = most_frequent_pubtypes(pubtypes)\n\n\nThere are 64397 instances of publication type in the dataset.\nThere are 6 different publication types in the dataset."
  },
  {
    "objectID": "types.html#visualization",
    "href": "types.html#visualization",
    "title": "Types",
    "section": "Visualization",
    "text": "Visualization\n\n\nCode\ndef visualize_pubtypes_counts(pubtypes_counts): \n    #print(pubtypes_counts)\n    plt.figure(figsize=(12,6))\n    pal = sns.color_palette(\"colorblind\", len(pubtypes_counts))\n    fig = sns.barplot(\n        data=pubtypes_counts, \n        y=\"types\", \n        x=\"count\", \n        orient='h' \n        )\n    for i in fig.containers:\n        fig.bar_label(i,)\n    plt.tight_layout()\n    plt.savefig(\n        join(wdir, \"figures\", \"pubtypes_counts.png\"),\n        dpi=300\n        )\n    \nvisualize_pubtypes_counts(pubtypes_counts)\n\n\n\n\n\nNote that, unfortunately, Zotero does not distinguish between the item types monograph and edited volume, both being treated as books. (The rule-based disambiguation of these item types, based on the presence of authors and/or editors, is future work.)"
  }
]