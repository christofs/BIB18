[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BIB18",
    "section": "",
    "text": "BIB18 – Curation and analyses of the dataset derived from the XVIIIe: Bibliographie"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "BIB18",
    "section": "Overview",
    "text": "Overview\nSome analyses of the dataset are available on the present website. The analyses are based on the RDF version of the data (currently, v0.1.0, May 20, 2023) and have been performed by Christof Schöch. Note that all numbers can be expected to shift slighly as the process of cleaning the data proceeds. Collaborative authorship / editorship as well as the publication types will most likely be most affected by corrections. The scripts used to create the analyses are integrated into the pages on this website for inspection (click on Code), but are also available as Jupyter Notebooks at github.com/christofs/BIB18.\nSource of the data: Benoît Melançon, for background information see mapageweb.umontreal.ca/melancon/donnees_biblios_1_550.html and for the Dataverse deposit, see doi.org/10.5683/SP3/PYYEEH.\nFor more information on the transformation process, and to obtain the dataset in various formats (BibTeX, Zotero RDF, JSON), see: github.com/christofs/BIB18. The transformation was performed in May 2023.\nThe full bibliography can also be consulted on Zotero, see: zotero.org/groups/5067408/BIB18."
  },
  {
    "objectID": "index.html#some-remarks-on-the-data",
    "href": "index.html#some-remarks-on-the-data",
    "title": "BIB18",
    "section": "Some remarks on the data",
    "text": "Some remarks on the data\n\nFor publication dates, only the year has been used.\nCorrect ordering of first and last names of authors or publishers is difficult, so errors may occur.\nThe data source does not distinguish between URLs and DOIs; consequently, they are all listed in the URL field. However, few DOIs are present anyway.\nInformation on the language of a publication has been added automatically, using the lingua-py module. This information can be erroneous in some cases (and will be improved by hand).\nAfter transformation into BibTex, the BibTeX-tidy tool was used: https://flamingtempura.github.io/bibtex-tidy.\nThe resulting files are available in the archive folder and form the basis for import into Zotero.\nSeveral Zotero export formats are available in the data folder.\nFuture corrections to the data will only be made on Zotero, with updates to the formats in the datafolder."
  },
  {
    "objectID": "index.html#nota-bene",
    "href": "index.html#nota-bene",
    "title": "BIB18",
    "section": "Nota bene",
    "text": "Nota bene\nBenoît Melançon specifies: “Quiconque souhaite s’approprier ces données peut le faire, sous deux conditions. 1. L’attribution de la collecte des données doit toujours être rappelée, par exemple sous la forme «Données colligées par Benoît Melançon». 2. Aucune exploitation commerciale de ces données n’est tolérée. Elles ne peuvent pas être vendues sous quelque forme que ce soit. Autrement dit, chez Creative Commons : Attribution-NonCommercial 4.0 International (CC BY-NC 4.0) — https://creativecommons.org/licenses/by-nc/4.0/.”"
  },
  {
    "objectID": "index.html#citation-suggestion",
    "href": "index.html#citation-suggestion",
    "title": "BIB18",
    "section": "Citation suggestion",
    "text": "Citation suggestion\nChristof Schöch. BIB18 – Curation and Analysis of the dataset derived from the XVIIIe: Bibliographie, v0.3.0. Github.com, 2023. URL: https://christofs.github.io/BIB18/, DOI: 10.5281/zenodo.8166110."
  },
  {
    "objectID": "index.html#notes-on-work-in-progress",
    "href": "index.html#notes-on-work-in-progress",
    "title": "BIB18",
    "section": "Notes on work in progress",
    "text": "Notes on work in progress\nThis website is work in progress. All bugs, todos, ideas for improvement are listed in the issue tracker. Feel free to add issues if you notice something!"
  },
  {
    "objectID": "coeditors.html",
    "href": "coeditors.html",
    "title": "Co-editors",
    "section": "",
    "text": "This page focuses on co-editorship, especially of collected volumes, in the Bibliographie.\nCode\n# === Imports === \n\nimport re \nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom os.path import join, abspath, normpath, realpath\nimport os\nfrom lxml import etree\nfrom io import StringIO, BytesIO\nfrom collections import Counter\nimport pandas as pd\nimport netgraph\nimport numpy as np\nfrom IPython.display import HTML, display   \nimport networkx as nx\nfrom pyvis.network import Network \nimport matplotlib.pyplot as plt\n\n\n# === Namespaces === \n\nnamespaces = {\n    \"foaf\" : \"http://xmlns.com/foaf/0.1/\",\n    \"bib\" : \"http://purl.org/net/biblio#\",\n    \"dc\" : \"http://purl.org/dc/elements/1.1/\",\n    \"z\" : \"http://www.zotero.org/namespaces/export#\",\n    \"rdf\" : \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n    }\n\n\n# === Files and parameters === \n\nwdir = join(\"/\", \"media\", \"christof\", \"Data\", \"Github\", \"christofs\", \"BIB18\")\nbibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF_2024-01-02.rdf\") \n#bibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF_TEST.rdf\") \n#graphviz_file = join(wdir, \"figures\", \"collaboration_coeditors-network.svg\") \n\n\n# === Load the dataset === \n\ndef read_xml(bibdatafile): \n    bibdata = etree.parse(bibdatafile)\n    return bibdata\n\nbibdata = read_xml(bibdatafile)"
  },
  {
    "objectID": "coeditors.html#number-of-collaborators-in-co-editorship",
    "href": "coeditors.html#number-of-collaborators-in-co-editorship",
    "title": "Co-editors",
    "section": "Number of collaborators in co-editorship",
    "text": "Number of collaborators in co-editorship\nThis section determines how frequent co-editorship is.\nMultiple entries of the same edited volume are taken into account only once, in this iteration of the analysis. (This avoids an undue inflation of the number of cases of co-editorship in cases where multiple chapters from the same edited volume are present in the database.) However, at the moment, this comes at the expense of the edited books not being included in the analysis. These will be added next.\n\n\nCode\ndef get_editordata(bibdata): \n    \"\"\"\n    Collects data on the titles and editors of all collected volumes for which the dataset includes book sections (like chapters).\n    Simplifies the data to just unique titles of collected volumes with their editors.\n    \"\"\"\n    # Find all instances of book chapters or book sections\n    xpath = \"//bib:BookSection\"\n    sections = bibdata.xpath(xpath, namespaces=namespaces)\n    print(\"There are \" + str(len(sections)) +  \" instances of element 'BookSection'.\")\n\n    # Also, find all instances of books mentioned as a whole and that have editors. \n    # === TBD ===\n\n    # For each section (and each book), get the editor(s), booktitle, authors, sectiontitle\n    editordata = {}\n    counter = 0\n    for sec in sections[:]: \n        counter +=1\n        xpath = \".//bib:editors//foaf:surname//text()\"\n        editors = sec.xpath(xpath, namespaces=namespaces)\n        editors = [editor for editor in editors if \"lacollaborationde\" not in editor]\n        xpath = \".//bib:Book/dc:title//text()\"\n        try: \n            booktitle = sec.xpath(xpath, namespaces=namespaces)[0]\n        except: \n            booktitle = \"missing title number \" + str(counter)\n            #print(booktitle) # Only happens less than 10 times, it seems. \n        # Collect each booktitle only once; only last occurrence is kept (!)\n        editordata[booktitle] = editors\n    # Check results \n    print(\"All of the book sections mentioned above correspond to (only)\", len(editordata), \"different titles of edited volumes.\")\n    ratio = np.round(np.divide(len(sections), len(editordata)),2)\n    print(\"This means that, on average, an edited volume is mentioned\", ratio, \"times, each mention corresponding to one section of the volume being mentioned.\")\n\n    #for title,editors in editordata.items(): \n    #    print(editors,title)\n    return editordata\neditordata = get_editordata(bibdata)\n\n\ndef get_coeditor_numbers(editordata): \n    \"\"\"\n    Based on the editordata, establish the number of times each number of co-editors appears, \n    as well as the corresponding percentages for each number of co-editors. \n    \"\"\"\n    coeditors = Counter([len(item) for item in editordata.values()])\n    coeditors = pd.DataFrame(pd.Series(coeditors, name=\"count\"))\n    coeditors[\"percentage\"] = np.round(np.multiply(np.divide(coeditors[\"count\"], np.sum(coeditors[\"count\"])),100),1)\n    # Prettify the DataFrame\n    coeditors[\"number\"] = coeditors.index\n    coeditors = coeditors.sort_values(by=\"number\")\n    coeditors = coeditors[['number', 'count', 'percentage']]\n    print(\"\\nThe following table shows counts and percentages for different numbers of co-editors.\\n\")\n    display(HTML(coeditors.to_html(index=False)))\n    return coeditors\ncoeditors = get_coeditor_numbers(editordata)\n\n\ndef visualize_coeditor_numbers(coeditors): \n    \"\"\"\n    Create a simple bar plot that shows \n    the percentage of each number of co-editors in the dataset.\n    \"\"\"\n    print(\"\\nThe following visualization shows counts and percentages for different numbers of co-editors.\\n\")\n\n    import seaborn.objects as so\n    (\n        so.Plot(data=coeditors, x=\"number\", y=\"count\", text=\"percentage\")\n        .add(so.Bar())\n        .add(so.Text(color=\"w\", valign=\"top\", offset=5))\n        .scale(x=so.Continuous().tick(every=1))\n        .label(\n            x=\"Number of co-editors\",\n            y=\"Counts (percentage shown as labels)\",\n            title = \"Counts and percentages of different number of co-editors\")\n        .save(\"figures/coeditor-percentages.png\", dpi=300)\n        .show()\n    )\nvisualize_coeditor_numbers(coeditors)\n\n\n############## Note from January 3, 2024 #############\n# Attention! For the moment, this data only looks at the book titles and editors for book sections. \n# However, there are also many \"books\" with editors (so not monographs), and these need to be taken into account as well.\n# What the results show already, compared to the earlier results, is that there is indeed a massive shift visible because of the (correct) aggregation of the editor data to the unique book titles. \n# It will be interesting to see what happens to the co-editorship network, I expect it to shift as well. \n#########################################################\n\n\nThere are 14421 instances of element 'BookSection'.\nAll of the book sections mentioned above correspond to (only) 2825 different titles of edited volumes.\nThis means that, on average, an edited volume is mentioned 5.1 times, each mention corresponding to one section of the volume being mentioned.\n\nThe following table shows counts and percentages for different numbers of co-editors.\n\n\nThe following visualization shows counts and percentages for different numbers of co-editors.\n\n\n\n\n\n\nnumber\ncount\npercentage\n\n\n\n\n0\n352\n12.5\n\n\n1\n923\n32.7\n\n\n2\n1068\n37.8\n\n\n3\n359\n12.7\n\n\n4\n101\n3.6\n\n\n5\n17\n0.6\n\n\n6\n5\n0.2\n\n\n\n\n\n\n\n\nCompared with the data on co-authorship (of books, articles and chapters), the figure above shows that co-editorship works quite differently. Indeed, when looking at editorship (of edited volumes or editions), joint editorship with two editors is the most common case (with around 38% of the cases). Single editorship, however, is also widespread (with 32%) and triple co-editorship not uncommon (at 12.7%).\nIt could be interesting to distinguish between editorship of edited volumes on the one hand, and textual editions, on the other hand. This is future work."
  },
  {
    "objectID": "coeditors.html#coeditor-pairs",
    "href": "coeditors.html#coeditor-pairs",
    "title": "Co-editors",
    "section": "Coeditor pairs",
    "text": "Coeditor pairs\nLooks at which people have frequently collaborated as editors of edited volumes and/or editions.\n\n\nCode\nimport warnings\nimport pandas as pd\nfrom pandas.core.common import SettingWithCopyWarning\nwarnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n\n\ndef get_recurring_coeditors(editordata): \n    \"\"\"\n    Which people have frequently collaborated as editors? \n    This is based on the data collected above (editors of unique titles). \n    \"\"\"\n\n    # Define filenames for output. \n    coeditorcounts_top_file = join(wdir, \"results\", \"coeditor-counts_top.csv\")\n    coeditorcounts_full_file = join(wdir, \"results\", \"coeditor-counts_full.csv\")\n\n    # Get editor names from editordata. \n    coeditors = editordata.values()\n    print(\"Overall\", len(coeditors), \"publications with editors are considered here.\")\n\n    # Establish the count of each collaboration between editors\n    import itertools \n    all_coeditor_combinations = []\n    for item in coeditors: \n        coeditor_combinations = list(itertools.combinations(item, 2))\n        coeditor_combinations = [tuple(sorted(item)) for item in coeditor_combinations]\n        for coedcomb in coeditor_combinations: \n            all_coeditor_combinations.append(coedcomb)\n    ccc = dict(Counter(all_coeditor_combinations)) # ccc = coeditor_combinations_count\n\n    # Transform to a DataFrame\n    ccc = pd.DataFrame.from_dict(ccc, orient=\"index\", columns=[\"count\"])\n    ccc = ccc.reset_index()\n    ccc_split = pd.DataFrame(ccc[\"index\"].tolist())\n    ccc_merged = ccc_split.merge(ccc, left_index=True, right_index=True)\n    ccc = ccc_merged.drop([\"index\"], axis=1)\n    ccc = ccc.rename({0 : \"coeditor1\", 1 : \"coeditor2\"}, axis=1)\n    ccc = ccc.sort_values(by=\"count\", ascending=False)\n    #print(ccc.head())\n    #print(ccc.shape, \"shape of dataframe\")\n    with open(join(coeditorcounts_full_file), \"w\", encoding=\"utf8\") as outfile: \n        ccc.to_csv(outfile, sep=\";\")\n\n    # Filter the DataFrame to make it manageable for visualization\n    # Determine the top N most frequent co-editors\n    coeditors_top = list(set(list(ccc.head(20).loc[:,\"coeditor1\"]) +\\\n        list(ccc.head(20).loc[:,\"coeditor2\"])))\n    #print(coeditors_top)\n    print(\"Among all editors, \" + str(len(coeditors_top)) + \" have been selected as the most active co-editors.\")\n    # Filter the DataFrame to include just the collaborations involving at least one of the top co-editors. \n    # The resulting DataFrame will have all collaborations between the top co-editors and their co-editors. \n    ccc_filtered = ccc[(ccc[\"coeditor1\"].isin(coeditors_top)) |\\\n                       (ccc[\"coeditor2\"].isin(coeditors_top))]\n    #print(ccc_filtered.shape, \"shape of dataframe of top co-editors and their co-editors.\")\n    # Simplify the labels \n    #ccc_filtered = ccc_filtered.replace(' .*?]', '',regex=True).astype(str)\n    ccc_filtered.loc[:,'coeditor1'] =  [re.sub(r', .*','', str(x)) for x in ccc_filtered.loc[:,'coeditor1']]\n    ccc_filtered.loc[:,'coeditor2'] =  [re.sub(r', .*','', str(x)) for x in ccc_filtered.loc[:,'coeditor2']]\n    \n    print(\"The following table shows the 12 most active pairs of editors.\\n\")\n    from IPython.display import HTML, display   \n    display(HTML(ccc_filtered.head(12).to_html(index=False)))\n\n    with open(join(coeditorcounts_top_file), \"w\", encoding=\"utf8\") as outfile: \n        ccc_filtered.to_csv(outfile, sep=\";\")\n    return ccc_filtered\n\nccc_filtered = get_recurring_coeditors(editordata)\n\n\nOverall 2825 publications with editors are considered here.\nAmong all editors, 32 have been selected as the most active co-editors.\nThe following table shows the 12 most active pairs of editors.\n\n\n\n\n\n\ncoeditor1\ncoeditor2\ncount\n\n\n\n\nHerman\nPelckmans\n12\n\n\nHasquin\nMortier\n9\n\n\nBiard\nLeuwers\n5\n\n\nBiard\nBourdin\n4\n\n\nHerman\nPeeters\n4\n\n\nFaggion\nRegina\n4\n\n\nBardet\nRuggiu\n4\n\n\nBerchtold\nMartin\n4\n\n\nPorret\nRosset\n4\n\n\nDonato\nLüsebrink\n4\n\n\nMcCallam\nPratt\n4\n\n\nPeeters\nPelckmans\n4"
  },
  {
    "objectID": "coeditors.html#co-editor-network",
    "href": "coeditors.html#co-editor-network",
    "title": "Co-editors",
    "section": "Co-editor network",
    "text": "Co-editor network\nBased on the frequency of co-editorships, it is possible to draw a network representation.\n\n\nCode\ndef create_plot(ccc): \n    \"\"\" \n    Plot the co-editor data as a network using pyvis. \n    \"\"\"\n    # Prepare the dataset\n    ccc = ccc_filtered\n    ccc.rename(columns={\"count\": \"weight\"}, inplace=True)\n    ccc = ccc[ccc[\"weight\"] &gt; 0]\n\n    # Load the data into a NetworkX graph\n    net = Network('1600px', '2800px', notebook=True, cdn_resources='in_line')\n    G = nx.from_pandas_edgelist(\n        ccc,\n        source = \"coeditor1\",\n        target = \"coeditor2\",\n        edge_attr = \"weight\",\n        )\n    print(G)\n\n    G = nx.Graph()\n    for line in ccc.iterrows(): \n        G.add_edge(line[1][0], line[1][1], weight=line[1][2], title=line[1][2])\n\n    \n    # Plot the data using pyvis\n    net.from_nx(G)\n    net.toggle_physics(True)\n    net.show(\"figures/coeditor-network.html\")\ncreate_plot(ccc)\n\n\nGraph with 154 nodes and 186 edges\nfigures/coeditor-network.html\n\n\nThis data can be visualized as a network. Click on the following image for an interactive network plot.\n\n\n\ncoeditor network\n\n\nThe visualization shows, like the tabular display, that Herman and Pelckmans are the most intense collaborators in terms of co-editorship.\nMore interestingly, however, the network visualisation shows that there are multiple, independent co-editor networks (when looking only at the subset of the most active co-editors). Some of them are loosely connected between each other, others not at all."
  },
  {
    "objectID": "coeditors.html#for-comparison-gephi-visualization",
    "href": "coeditors.html#for-comparison-gephi-visualization",
    "title": "Co-editors",
    "section": "For comparison: Gephi visualization",
    "text": "For comparison: Gephi visualization\nSee the collaborations page. – NB! Note that the analysis and visualizations there are currently based on the data before reduction to unique book titles."
  },
  {
    "objectID": "collaborations.html",
    "href": "collaborations.html",
    "title": "Collaborations",
    "section": "",
    "text": "This page focuses on aspects of collaboration, like co-authorship or co-editorship, in the Bibliographie sur le XVIIIe siècle.\nCode\n# === Imports === \n\nimport re \nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom os.path import join, abspath, normpath, realpath\nimport os\nfrom lxml import etree\nfrom io import StringIO, BytesIO\nfrom collections import Counter\nimport pandas as pd\nimport netgraph\n\n\n# === Namespaces === \n\nnamespaces = {\n    \"foaf\" : \"http://xmlns.com/foaf/0.1/\",\n    \"bib\" : \"http://purl.org/net/biblio#\",\n    \"dc\" : \"http://purl.org/dc/elements/1.1/\",\n    \"z\" : \"http://www.zotero.org/namespaces/export#\",\n    \"rdf\" : \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n    }\n\n\n# === Files and parameters === \n\nwdir = join(\"/\", \"media\", \"christof\", \"Data\", \"Github\", \"christofs\", \"BIB18\")\nbibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF.rdf\") \n#bibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF_TEST.rdf\") \ngraphviz_file = join(wdir, \"figures\", \"collaboration_coeditors-network.svg\") \n\n\n# === Load the dataset === \n\ndef read_xml(bibdatafile): \n    bibdata = etree.parse(bibdatafile)\n    return bibdata\n\nbibdata = read_xml(bibdatafile)"
  },
  {
    "objectID": "collaborations.html#number-of-collaborators-in-co-authorship-and-co-editorship",
    "href": "collaborations.html#number-of-collaborators-in-co-authorship-and-co-editorship",
    "title": "Collaborations",
    "section": "Number of collaborators in co-authorship and co-editorship",
    "text": "Number of collaborators in co-authorship and co-editorship\nThis section determines how frequent collaborations (co-authorship, co-editorship) are. This is based on the number of “Person” elements within the “authors” or “editors” element.\nNote an important caveat of the data: Any time a book section in an edited volume is cited, the editors of this volume are included in the dataset. This unduly inflates the number of cases of co-editorship and will need to be resolved in a future iteration of this research by counting any edited volume only once, independently of the number of chapters from the volume mentioned in the dataset.\n\n\nCode\ndef get_number_collaborators(bibdata): \n    \"\"\"\n    Finds out how frequent collaborations (co-authorship, co-editorship) are.\n    Number of \"Person\" elements within \"authors\" or \"editors\" element. \n    \"\"\"\n    # Find all instances of authors\n    num_coauthors = []\n    xpath = \"//bib:authors\"\n    authors = bibdata.xpath(xpath, namespaces=namespaces)\n    print(\"There are \" + str(len(authors)) +  \" instances of Element 'authors' in the dataset.\")\n    num_coauthors = []\n    for item in authors:\n        #print(item)\n        xpath = \"rdf:Seq/rdf:li/foaf:Person\"\n        coauthors = item.xpath(xpath, namespaces=namespaces)\n        num_coauthors.append(len(coauthors))\n    num_coauthors_counts = Counter(num_coauthors)\n    #print(num_coauthors_counts)\n\n    # Calculate percentages\n    num_coauthors_perc = {}\n    total = sum(num_coauthors_counts.values())\n    for key,val in num_coauthors_counts.items():\n        #num_coauthors_perc[key] = str(round(val/total * 100, 3)) + '%'\n        num_coauthors_perc[key] = round(val/total * 100, 1)\n    #print(num_coauthors_perc)\n\n    # Find all instances of editors\n    num_coeditors = []\n    xpath = \"//bib:editors\"\n    editors = bibdata.xpath(xpath, namespaces=namespaces)\n    print(\"In addition, there are \" + str(len(editors)) +  \" instances of Element 'editors' in the dataset.\")\n    num_coeditors = []\n    for item in editors:\n        xpath = \"rdf:Seq/rdf:li/foaf:Person\"\n        coeditors = item.xpath(xpath, namespaces=namespaces)\n        num_coeditors.append(len(coeditors))\n    num_coeditors_counts = Counter(num_coeditors)\n    #print(dict(num_coeditors_counts))\n\n    # Calculate percentages\n    num_coeditors_perc = {}\n    total = sum(num_coeditors_counts.values())\n    for key,val in num_coeditors_counts.items():\n        #num_coeditors_perc[key] = str(round(val/total * 100, 2)) + '%'\n        num_coeditors_perc[key] = round(val/total * 100, 1)\n    #print(num_coeditors_perc)\n    \n    return num_coauthors_perc, num_coeditors_perc\n\n\n\ndef visualize_coauthorship(coauthors, coeditors): \n    # Prepare the data \n    coauthors = pd.Series(coauthors)\n    #print(coauthors)\n    coeditors = pd.Series(coeditors)\n    #print(coeditors)\n    collaborations = pd.concat([coauthors, coeditors], axis=1)\n    collaborations.rename(columns={0 : \"coauthors\", 1 : \"coeditors\"}, inplace=True)\n    collaborations = collaborations.fillna(0)\n    #print(collaborations)\n    \n    import numpy as np\n    fig, ax = plt.subplots(layout='constrained', figsize = (10,6))\n    X_axis = np.arange(7)\n    Y_axis = np.arange(7)\n    collaboration_labels = [\"single\", \"dual\", \"three\", \"four\", \"five\", \"six\", \"... eleven\"]\n    plt.bar(X_axis - 0.2, collaborations[\"coauthors\"], 0.4, label = 'coauthors')\n    plt.bar(Y_axis + 0.2, collaborations[\"coeditors\"], 0.4, label = 'coeditors')\n    plt.ylim([0, 100])\n    plt.xlabel(\"Number of coauthors / coeditors\")\n    plt.ylabel(\"Percentage\")\n    plt.title(\"Percentages of coauthorships and coeditorships\")    \n    ax.set_xticks(np.arange(7), collaboration_labels)\n    ax.legend(loc='upper right')\n    \n    # function to add value labels\n    def addlabels_a(x,y):\n        for i in range(len(x)):\n            if x[i] in [\"single\", \"dual\", \"three\"]: \n                plt.text(i, y[i], y[i], ha = \"right\")\n    def addlabels_e(x,y):\n        for i in range(len(x)):\n            plt.text(i, y[i], y[i], ha = \"left\")\n    addlabels_a(collaboration_labels, list(collaborations[\"coauthors\"]))\n    addlabels_e(collaboration_labels, list(collaborations[\"coeditors\"]))\n    \n\n    \ncoauthors, coeditors = get_number_collaborators(bibdata) \nvisualize_coauthorship(coauthors, coeditors)\n\n\nThere are 56860 instances of Element 'authors' in the dataset.\nIn addition, there are 17135 instances of Element 'editors' in the dataset.\n\n\n\n\n\nRegarding authorship (of books, articles and chapters), the figure above shows that single authorship is clearly the norm in this dataset (with around 94% of the cases). The remaining 6% are cases of dual authorship. There are no cases of multiple authorship beyond the constellation of dual authorship.\nIn contrast, when looking at editorship (of edited volumes or editions), joint editorship with two editors is the most common case (with around 45% of the cases). Single editorship, however, is also widespread (with 32%) and triple co-editorship not uncommon (with 15%). There is one notable outlier in the dataset, with a title being edited by 11 co-editors.\nIt could be interesting to distinguish between editorship of edited volumes on the one hand, and textual editions, on the other hand. This is future work."
  },
  {
    "objectID": "collaborations.html#coeditor-pairs",
    "href": "collaborations.html#coeditor-pairs",
    "title": "Collaborations",
    "section": "Coeditor pairs",
    "text": "Coeditor pairs\nBuilds the data for a network of people having collaborated as editors.\n\n\nCode\nimport warnings\nimport pandas as pd\nfrom pandas.core.common import SettingWithCopyWarning\nwarnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n\n\ncoeditorcounts_top_file = join(wdir, \"results\", \"coeditor-counts_top.csv\")\ncoeditorcounts_full_file = join(wdir, \"results\", \"coeditor-counts_full.csv\")\n\n\ndef network_coeditors(bibdata): \n    \"\"\"\n    Builds the data for a network of people having collaborated as editors. \n    \"\"\"\n    # Find all instances of editors\n    xpath = \"//bib:editors\"\n    editors = bibdata.xpath(xpath, namespaces=namespaces)\n    print(\"There are \" + str(len(editors)) + \" editors (i.e., instances of Element 'editors').\")\n\n    # Collect the names of each person within each editors element\n    coeditors = []\n    for item in editors:\n        xpath = \"rdf:Seq/rdf:li/foaf:Person\"\n        coeditors_elements = item.xpath(xpath, namespaces=namespaces)\n        coeditors_names = []\n        # Get the names (full name or first name, last name) from each person\n        for item in coeditors_elements: \n            if len(item) == 2: \n                coeditors_names.append(item[0].text + \", \" + item[1].text)\n        coeditors.append(coeditors_names)\n\n    # Establish the count of each collaboration between editors\n    import itertools \n    all_coeditor_combinations = []\n    for item in coeditors: \n        coeditor_combinations = list(itertools.combinations(item, 2))\n        coeditor_combinations = [tuple(sorted(item)) for item in coeditor_combinations]\n        for coedcomb in coeditor_combinations: \n            all_coeditor_combinations.append(coedcomb)\n    ccc = dict(Counter(all_coeditor_combinations)) # ccc = coeditor_combinations_count\n\n    # Transform to a DataFrame\n    ccc = pd.DataFrame.from_dict(ccc, orient=\"index\", columns=[\"count\"])\n    ccc = ccc.reset_index()\n    ccc_split = pd.DataFrame(ccc[\"index\"].tolist())\n    ccc_merged = ccc_split.merge(ccc, left_index=True, right_index=True)\n    ccc = ccc_merged.drop([\"index\"], axis=1)\n    ccc = ccc.rename({0 : \"coeditor1\", 1 : \"coeditor2\"}, axis=1)\n    ccc = ccc.sort_values(by=\"count\", ascending=False)\n    #print(ccc.head())\n    #print(ccc.shape, \"shape of dataframe\")\n    with open(join(coeditorcounts_full_file), \"w\", encoding=\"utf8\") as outfile: \n        ccc.to_csv(outfile, sep=\";\")\n\n    # Filter the DataFrame to make it manageable for visualization\n    # Determine the top N most frequent co-editors\n    coeditors_top = list(set(list(ccc.head(20).loc[:,\"coeditor1\"]) +\\\n        list(ccc.head(20).loc[:,\"coeditor2\"])))\n    #print(coeditors_top)\n    print(\"Among all editors, \" + str(len(coeditors_top)) + \" have been selected as the most active co-editors.\")\n    # Filter the DataFrame to include just the collaborations involving at least one of the top co-editors. \n    # The resulting DataFrame will have all collaborations between the top co-editors and their co-editors. \n    ccc_filtered = ccc[(ccc[\"coeditor1\"].isin(coeditors_top)) |\\\n                       (ccc[\"coeditor2\"].isin(coeditors_top))]\n    #print(ccc_filtered.shape, \"shape of dataframe of top co-editors and their co-editors.\")\n    # Simplify the labels \n    #ccc_filtered = ccc_filtered.replace(' .*?]', '',regex=True).astype(str)\n    ccc_filtered.loc[:,'coeditor1'] =  [re.sub(r', .*','', str(x)) for x in ccc_filtered.loc[:,'coeditor1']]\n    ccc_filtered.loc[:,'coeditor2'] =  [re.sub(r', .*','', str(x)) for x in ccc_filtered.loc[:,'coeditor2']]\n    \n    print(\"The following table shows the 5 most active pairs of editors.\\n\")\n    from IPython.display import HTML, display   \n    display(HTML(ccc_filtered.head().to_html(index=False)))\n\n    with open(join(coeditorcounts_top_file), \"w\", encoding=\"utf8\") as outfile: \n        ccc_filtered.to_csv(outfile, sep=\";\")\n\nnetwork_coeditors(bibdata)\n\n\nThere are 17135 editors (i.e., instances of Element 'editors').\nAmong all editors, 20 have been selected as the most active co-editors.\nThe following table shows the 5 most active pairs of editors.\n\n\n\n\n\n\ncoeditor1\ncoeditor2\ncount\n\n\n\n\nHerman\nPelckmans\n221\n\n\nHasquin\nMortier\n103\n\n\nFerreyrolles\nVersini\n84\n\n\nBiard\nLeuwers\n78\n\n\nBiard\nBourdin\n64"
  },
  {
    "objectID": "collaborations.html#co-editor-networks",
    "href": "collaborations.html#co-editor-networks",
    "title": "Collaborations",
    "section": "Co-editor networks",
    "text": "Co-editor networks\nThe data generated above has been imported to Gephi to create the network visualizations that follow.\n(The code block present here is work in progress with the aim to use the Python library Netgraph for these visualisations instead of an external tool like Gephi, to improve transparency and reproducibility.)\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom netgraph import Graph\n\ndef load_data(coeditorcounts_top_file):\n    # Load the data from file\n    with open(join(coeditorcounts_top_file), \"r\", encoding=\"utf8\") as infile: \n        ccc = pd.read_csv(infile, sep=\";\", index_col=0)\n    #print(ccc.head())\n    return ccc\n\n\ndef create_plot(ccc): \n    # Get edges (without weights)\n    edges = ccc.loc[:,[\"coeditor1\", \"coeditor2\"]].to_records(index=False).tolist()\n    #print(edges)\n    weights = ccc.loc[:,\"count\"].tolist()\n    #print(weights)\n    edgeweights = dict(zip(edges, weights))\n    #print(edgeweights)\n           \n    # Calculate the layout\n    positions = netgraph.get_fruchterman_reingold_layout(\n        edges, \n        edgeweights, \n        k = None, \n        origin = (0, 0), \n        scale = (2, 2), \n        pad_by = 0.05, \n        initial_temperature = 2.0, \n        total_iterations = 100, \n        node_size = 0.5, \n        node_positions = None, \n        fixed_nodes = None\n        )\n\n    # Plot the data\n    ccc = ccc.to_records(index=False).tolist() # Transform to list of tuples\n    fig, ax = plt.subplots(figsize=(20, 20))\n    cmap = 'RdGy'\n    Graph(\n        ccc, \n        edge_cmap = cmap, \n        edge_width = 2., \n        arrows = False,\n        node_labels = True,\n        node_label_offset = 0.005,\n        node_layout = positions,\n        edge_layout = 'bundled',\n        \n        node_label_fontdict = dict(\n            #fontweight=\"bold\",\n            fontsize = 8,\n            )\n        )\n    plt.savefig(graphviz_file, dpi=300)\n    \n\n\nccc = load_data(coeditorcounts_top_file)\ncreate_plot(ccc)\n\n\nAs shown above, editorship is an area of particularly intense collaboration in the community of Dix-huitiémistes, based on the data in the bibliography. The following is an initial, experimental attempt to elucidate the data using network visualization.\nThe following shows a network of the top 20 co-editors and all of their co-editors, resulting in 148 different co-editor pairs. Each node is one editor, and each time two people have co-edited a publication, a link between them is created. The more co-editorships a person accumulates, the larger the node. The more co-editorships two people share, the thicker the edge connecting them. For this visualization, the full data of collaborations for edited volumes and editions has been massively reduced. Different parameters may strongly affect the results. See the full coeditor data in the data folder for more details.\n\n\n\nNetwork showing the top 20 co-editors and all of their co-editors, created using Gephi.\n\n\nThis figure is available also as a zoom-able image file and with somewhat friendlier colors but no community detection.\nWe basically see three key co-editor networks (the different colors are based on an algorithmic community or cluster detection):\n\nPorret, Rosset, Majeur, Farkas, Baczko et al. \nSermain, Herman, Pelckmans, Escola, Omacini, Peeters, Paschoud, Berchtold et al. \nLeuwers, Bourdin, Biard, Simien, Serna, Antoine et al. \nSmaller clusters with Didier and Neefs as well around Kolving and Mortier.\n\nSome initial observations: While Porret appears to be the most productive co-editor overall, this is achived with some frequent, but also with a large number of less frequent coeditors. Inversely, the most intense collaboration appears to be between Herman and Pelckmans, who seem to avoid one-off collaborations. Finally, Rosset also functions as a bridge linking Porret and Baczko on the one hand, and Herman and Pelckmans on the other hand, and their respective coeditor networks. No similar bridge exists towards the Leuwers et al. network. The smaller Didier cluster is also connected to Sermain."
  },
  {
    "objectID": "languages.html",
    "href": "languages.html",
    "title": "Languages",
    "section": "",
    "text": "This section provides some analyses of the languages of publications referenced in the Bibliographie.\nCode\n# === Imports === \n\nimport re \nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom os.path import join\nfrom os.path import realpath, dirname\nimport os\nfrom lxml import etree\nfrom io import StringIO, BytesIO\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\n\n\n# === Files and parameters === \n\nwdir = join(\"/\", \"media\", \"christof\", \"Data\", \"Github\", \"christofs\", \"BIB18\")\nbibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF.rdf\") \n#bibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF_TEST.rdf\") \n\nnamespaces = {\n    \"foaf\" : \"http://xmlns.com/foaf/0.1/\",\n    \"bib\" : \"http://purl.org/net/biblio#\",\n    \"dc\" : \"http://purl.org/dc/elements/1.1/\",\n    \"z\" : \"http://www.zotero.org/namespaces/export#\",\n    \"rdf\" : \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n    }\n\n# === Load the dataset === \n\ndef read_xml(bibdatafile): \n    bibdata = etree.parse(bibdatafile)\n    return bibdata\nbibdata = read_xml(bibdatafile)"
  },
  {
    "objectID": "languages.html#first-approach-language",
    "href": "languages.html#first-approach-language",
    "title": "Languages",
    "section": "First approach: ‘language’",
    "text": "First approach: ‘language’\nWe can use the “language” field in the dataset. This is precise, technically speaking, but depends on the level of curation of the dataset. The language field is not fully reliable at this time.\nWe don’t have abstracts or full texts in the dataset, but on the basis of the titles alone, the (most likely) language of the publication has been determined. Note that an algorithmic process, based on the library py-lingua and using only the sometimes very short titles, has been used to create this data, so errors are to be expected. With the progress of corrections in the dataset, this will improve over time.\n\n\nCode\ndef get_languages(bibdata): \n    print(\"\\nLanguages\")\n\n    # Find all the instances of \"language\" Element and its content\n    xpath = \"//z:language/text()\"\n    languages = bibdata.xpath(xpath, namespaces=namespaces)\n    # Identify frequency of languages\n    languages_counts = Counter(languages)\n    languages_counts = dict(sorted(languages_counts.items(), key = lambda item: item[1], reverse=True)[:10])\n    \n \n    # Visualize using a simple bar chart \n    lc = pd.DataFrame.from_dict(languages_counts, orient=\"index\", columns=[\"count\"]).reset_index().rename({\"index\" : \"language\"}, axis=1)\n    plt.figure(figsize=(12,6))\n    pal = sns.color_palette(\"colorblind\", len(lc))\n    fig = sns.barplot(data=lc, x=\"language\", y=\"count\", palette=pal)\n    fig.set_xticklabels(fig.get_xticklabels(), rotation=30)\n    for i in fig.containers:\n        fig.bar_label(i,)\n    plt.tight_layout()\n    plt.savefig(join(wdir, \"figures\", \"languages_counts.png\"), dpi=300)\n    return languages, languages_counts\n  \nlanguages, languages_counts = get_languages(bibdata)\n\n\n\n\nLanguages\n\n\n\n\n\n\n\nCode\n    \n# Provide some results as a text. \nprint(\"There are \" + str(len(languages))  + \" instances of language in the dataset.\")\nprint(\"At the moment, only \" + str(len(languages_counts)) + \" different languages are considered for analysis.\")\nlanguages_perc = {k: v / len(languages) for k, v in languages_counts.items()}\nprint(\"The most prevalent language is \" + str(list(languages_perc.keys())[0]) + \", with \" + \"{:2.2%}\".format(list(languages_perc.values())[0]) + \" of all entries.\")\n\n\n\n\nThere are 64396 instances of language in the dataset.\nAt the moment, only 8 different languages are considered for analysis.\nThe most prevalent language is French, with 73.89% of all entries."
  },
  {
    "objectID": "people.html",
    "href": "people.html",
    "title": "People",
    "section": "",
    "text": "This page provides information about person names, i.e. authors and editors.\n\n\nCode\n# === Imports === \n\nimport re \nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom os.path import join\nfrom os.path import realpath, dirname\nimport os\nfrom lxml import etree\nfrom io import StringIO, BytesIO\nfrom collections import Counter\nimport pandas as pd\n\n\n# === Files and parameters === \n\nwdir = join(\"/\", \"media\", \"christof\", \"Data\", \"Github\", \"christofs\", \"BIB18\")\nbibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF.rdf\") \n#bibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF_TEST.rdf\") \n\n\nnamespaces = {\n    \"foaf\" : \"http://xmlns.com/foaf/0.1/\",\n    \"bib\" : \"http://purl.org/net/biblio#\",\n    \"dc\" : \"http://purl.org/dc/elements/1.1/\",\n    \"z\" : \"http://www.zotero.org/namespaces/export#\",\n    \"rdf\" : \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n    }\n\n\ndef read_json(bibdatafile): \n    bibdata = etree.parse(bibdatafile)\n    return bibdata\n\nbibdata = read_json(bibdatafile)\n\n\n\n\nCode\ndef get_personnames(bibdata): \n\n    # Find all the instances of persons\n    personnames = []\n    xpath = \"//foaf:Person\"\n    persons = bibdata.xpath(xpath, namespaces=namespaces)\n    print(\"There are \" + str(len(persons)) + \" instances of the Element 'person' in the dataset.\")\n\n    # Get the names (full name or first name, last name) from each person\n    for item in persons: \n        if len(item) == 1: \n            personname = item[0].text\n            personnames.append(personname)\n        elif len(item) == 2: \n            personname = item[0].text + \", \" + item[1].text \n            personnames.append(personname)    \n    print(\"There are \" + str(len(Counter(personnames))) + \" different person names in the dataset.\")\n    return personnames\n\nglobal personnames\npersonnames  = get_personnames(bibdata)\n\n\nThere are 94321 instances of the Element 'person' in the dataset.\nThere are 31026 different person names in the dataset."
  },
  {
    "objectID": "people.html#most-frequently-occurring-person-names",
    "href": "people.html#most-frequently-occurring-person-names",
    "title": "People",
    "section": "",
    "text": "This page provides information about person names, i.e. authors and editors.\n\n\nCode\n# === Imports === \n\nimport re \nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom os.path import join\nfrom os.path import realpath, dirname\nimport os\nfrom lxml import etree\nfrom io import StringIO, BytesIO\nfrom collections import Counter\nimport pandas as pd\n\n\n# === Files and parameters === \n\nwdir = join(\"/\", \"media\", \"christof\", \"Data\", \"Github\", \"christofs\", \"BIB18\")\nbibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF.rdf\") \n#bibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF_TEST.rdf\") \n\n\nnamespaces = {\n    \"foaf\" : \"http://xmlns.com/foaf/0.1/\",\n    \"bib\" : \"http://purl.org/net/biblio#\",\n    \"dc\" : \"http://purl.org/dc/elements/1.1/\",\n    \"z\" : \"http://www.zotero.org/namespaces/export#\",\n    \"rdf\" : \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n    }\n\n\ndef read_json(bibdatafile): \n    bibdata = etree.parse(bibdatafile)\n    return bibdata\n\nbibdata = read_json(bibdatafile)\n\n\n\n\nCode\ndef get_personnames(bibdata): \n\n    # Find all the instances of persons\n    personnames = []\n    xpath = \"//foaf:Person\"\n    persons = bibdata.xpath(xpath, namespaces=namespaces)\n    print(\"There are \" + str(len(persons)) + \" instances of the Element 'person' in the dataset.\")\n\n    # Get the names (full name or first name, last name) from each person\n    for item in persons: \n        if len(item) == 1: \n            personname = item[0].text\n            personnames.append(personname)\n        elif len(item) == 2: \n            personname = item[0].text + \", \" + item[1].text \n            personnames.append(personname)    \n    print(\"There are \" + str(len(Counter(personnames))) + \" different person names in the dataset.\")\n    return personnames\n\nglobal personnames\npersonnames  = get_personnames(bibdata)\n\n\nThere are 94321 instances of the Element 'person' in the dataset.\nThere are 31026 different person names in the dataset."
  },
  {
    "objectID": "people.html#visualization-of-the-most-frequent-person-names",
    "href": "people.html#visualization-of-the-most-frequent-person-names",
    "title": "People",
    "section": "Visualization of the most frequent person names",
    "text": "Visualization of the most frequent person names\nThese persons could be authors or editors of publications.\n\n\nCode\ndef most_frequent_persons(personnames):\n    # Count the occurrences, find the 10 most frequently mentioned publishers\n    personnames_counts = Counter(personnames)\n    personnames_counts = dict(\n        sorted(personnames_counts.items(), \n        key = lambda item: item[1], reverse=True)[:20]\n        )\n    \n    columns = [\"count\"]\n    personnames_counts = pd.DataFrame.from_dict(\n        personnames_counts, \n        orient=\"index\", \n        columns=[\"count\"]).reset_index().rename({\"index\" : \"person\"}, \n        axis=1\n        )\n    \n    #print(personnames_counts)\n    return personnames_counts\n\n\ndef visualize_personnames_counts(personnames_counts): \n    #print(publishernames_counts)\n    plt.figure(figsize=(12,8))\n    pal = sns.color_palette(\"colorblind\", len(personnames_counts))\n    fig = sns.barplot(\n        data=personnames_counts, \n        y=\"person\", \n        x=\"count\", \n        orient='h' \n        )\n    for i in fig.containers:\n        fig.bar_label(i,)\n    plt.tight_layout()\n    plt.savefig(\n        join(wdir, \"figures\", \"personnames_counts.png\"),\n        dpi=300\n        )\n\n\npersonnames_counts = most_frequent_persons(personnames)\nvisualize_personnames_counts(personnames_counts)\n\n\nThere are 31026 different person names in the dataset."
  },
  {
    "objectID": "publishers.html",
    "href": "publishers.html",
    "title": "Publishers",
    "section": "",
    "text": "This page provides information about the publishers mentioned in the dataset.\nCode\n# === Imports === \n\nimport re \nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom os.path import join\nfrom os.path import realpath, dirname\nimport os\nfrom lxml import etree\nfrom io import StringIO, BytesIO\nfrom collections import Counter\nimport pandas as pd\n\n\n# === Files and parameters === \n\nwdir = join(\"/\", \"media\", \"christof\", \"Data\", \"Github\", \"christofs\", \"BIB18\")\nbibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF.rdf\") \n#bibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF_TEST.rdf\") \n\n\nnamespaces = {\n    \"foaf\" : \"http://xmlns.com/foaf/0.1/\",\n    \"bib\" : \"http://purl.org/net/biblio#\",\n    \"dc\" : \"http://purl.org/dc/elements/1.1/\",\n    \"z\" : \"http://www.zotero.org/namespaces/export#\",\n    \"rdf\" : \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n    }\n\n\ndef read_json(bibdatafile): \n    bibdata = etree.parse(bibdatafile)\n    return bibdata\n\nbibdata = read_json(bibdatafile)\nCode\ndef get_publishers(bibdata): \n    # Find all the instances of publisher names\n    publishers = []\n    xpath = \"//dc:publisher//foaf:name/text()\"\n    publishers = bibdata.xpath(xpath, namespaces=namespaces)    \n    # Show some results\n    print(\"There are \" + str(len(publishers)) + \" instances of publishers mentioned in the dataset.\" )\n    print(\"There are a total of \" + str(len(set(publishers))) + \" different publishers mentioned in the dataset.\" )\n    return publishers\n\n\ndef most_frequent_publishers(publishers):\n    # Count the occurrences, find the 10 most frequently mentioned publishers\n    publishernames_counts = Counter(publishers)\n    publishernames_counts = dict(\n        sorted(publishernames_counts.items(), \n        key = lambda item: item[1], reverse=True)[:20]\n        )\n    \n    columns = [\"occurrences\"]\n    publishernames_counts = pd.DataFrame.from_dict(\n        publishernames_counts, \n        orient=\"index\", \n        columns=[\"count\"]).reset_index().rename({\"index\" : \"publisher\"}, \n        axis=1\n        )\n    return publishernames_counts\n\n\nglobal publishernames_counts\npublishers = get_publishers(bibdata)\npublishernames_counts = most_frequent_publishers(publishers)\n\n\nThere are 37781 instances of publishers mentioned in the dataset.\nThere are a total of 5705 different publishers mentioned in the dataset."
  },
  {
    "objectID": "publishers.html#visualization",
    "href": "publishers.html#visualization",
    "title": "Publishers",
    "section": "Visualization",
    "text": "Visualization\n\n\nCode\n\ndef visualize_publishername_counts(publishernames_counts): \n    #print(publishernames_counts)\n    plt.figure(figsize=(12,8))\n    pal = sns.color_palette(\"colorblind\", len(publishernames_counts))\n    fig = sns.barplot(\n        data=publishernames_counts, \n        y=\"publisher\", \n        x=\"count\", \n        orient='h' \n        )\n    for i in fig.containers:\n        fig.bar_label(i,)\n    plt.tight_layout()\n    plt.savefig(\n        join(wdir, \"figures\", \"publishernames_counts.png\"),\n        dpi=300\n        )\n    \nvisualize_publishername_counts(publishernames_counts)"
  },
  {
    "objectID": "time.html",
    "href": "time.html",
    "title": "Time",
    "section": "",
    "text": "Here, only data starting in 1986 is shown (the bibliography was launched in 1992).\n\n\nCode\n# === Imports === \n\nimport re \nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom os.path import join\nfrom os.path import realpath, dirname\nimport os\nfrom lxml import etree\nfrom io import StringIO, BytesIO\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\n\n\n# === Files and parameters === \n\nwdir = join(\"/\", \"media\", \"christof\", \"Data\", \"Github\", \"christofs\", \"BIB18\")\nbibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF.rdf\") \n#bibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF_TEST.rdf\") \n\nnamespaces = {\n    \"foaf\" : \"http://xmlns.com/foaf/0.1/\",\n    \"bib\" : \"http://purl.org/net/biblio#\",\n    \"dc\" : \"http://purl.org/dc/elements/1.1/\",\n    \"z\" : \"http://www.zotero.org/namespaces/export#\",\n    \"rdf\" : \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n    }\n\n# === Load the dataset === \n\ndef read_xml(bibdatafile): \n    bibdata = etree.parse(bibdatafile)\n    return bibdata\nbibdata = read_xml(bibdatafile)\n\n\n\n\nCode\n\ndef get_pubyears(bibdata): \n    # Setting things up\n    pubyears = []\n\n    # Find all the instances of publication dates\n    xpath = \"//dc:date/text()\"\n    pubyears = bibdata.xpath(xpath, namespaces=namespaces)\n\n    # Count the occurrences, find the 10 most frequently mentioned publication dates\n    pubyear_counts = Counter(pubyears)\n    pubyear_counts = dict(sorted(pubyear_counts.items(), reverse=False))\n\n    # Filter data to clean it\n    pubyear_counts = pd.DataFrame.from_dict(pubyear_counts, orient=\"index\").reset_index().rename(mapper={\"index\":\"year\", 0 : \"count\"}, axis=\"columns\")    #pubyear_counts = pubyear_counts[pubyear_counts[0] == 1991]\n    pubyear_counts = pubyear_counts[pubyear_counts[\"year\"].str.isnumeric()]\n    pubyear_counts.set_index(\"year\", inplace=True)\n    # Remove erroneous years (to be corrected in the data)\n    pubyear_counts.drop([\"134\", \"207\", \"22\", \"30\", \"42\", \"58\", \"76\", \"78\", \"20\", \"201\"], inplace=True)\n    # Remove less relevant years (optional, of course)\n    pubyear_counts.drop([\"1815\", \"1834\", \"1891\", \"1932\", \"1945\", \"1957\", \"1961\", \"1969\", \"1973\", \"1974\", \"1975\", \"1978\", \"1979\", \"1981\", \"1982\", \"1983\", \"1984\"], inplace=True)\n    pubyear_counts.reset_index(inplace=True)\n    #print(pubyear_counts.head())\n\n    # Display some key figures\n    print(\"There are \" + str(len(pubyears)) + \" instances of publication years mentioned in the dataset.\")\n    print(\"There are \" + str(len(pubyear_counts)) + \" different years mentioned in the dataset.\")\n\n    return pubyear_counts\n\n\ndef visualize_pubyears(pubyear_counts):\n    plt.figure(figsize=(12,8))\n    pal = sns.color_palette(\"colorblind\", len(pubyear_counts))\n    fig = sns.barplot(\n        data = pubyear_counts, \n        x=\"year\", \n        y=\"count\",\n        palette = pal,\n        )\n    fig.set_xticklabels(fig.get_xticklabels(), rotation=45)\n    plt.savefig(join(wdir, \"figures\", \"pubyear_counts.png\"), dpi=300)\n\n\npubyear_counts = get_pubyears(bibdata)\nvisualize_pubyears(pubyear_counts)\n\n\nThere are 64169 instances of publication years mentioned in the dataset.\nThere are 38 different years mentioned in the dataset."
  },
  {
    "objectID": "time.html#distribution-of-publications-per-year",
    "href": "time.html#distribution-of-publications-per-year",
    "title": "Time",
    "section": "",
    "text": "Here, only data starting in 1986 is shown (the bibliography was launched in 1992).\n\n\nCode\n# === Imports === \n\nimport re \nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom os.path import join\nfrom os.path import realpath, dirname\nimport os\nfrom lxml import etree\nfrom io import StringIO, BytesIO\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\n\n\n# === Files and parameters === \n\nwdir = join(\"/\", \"media\", \"christof\", \"Data\", \"Github\", \"christofs\", \"BIB18\")\nbibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF.rdf\") \n#bibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF_TEST.rdf\") \n\nnamespaces = {\n    \"foaf\" : \"http://xmlns.com/foaf/0.1/\",\n    \"bib\" : \"http://purl.org/net/biblio#\",\n    \"dc\" : \"http://purl.org/dc/elements/1.1/\",\n    \"z\" : \"http://www.zotero.org/namespaces/export#\",\n    \"rdf\" : \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n    }\n\n# === Load the dataset === \n\ndef read_xml(bibdatafile): \n    bibdata = etree.parse(bibdatafile)\n    return bibdata\nbibdata = read_xml(bibdatafile)\n\n\n\n\nCode\n\ndef get_pubyears(bibdata): \n    # Setting things up\n    pubyears = []\n\n    # Find all the instances of publication dates\n    xpath = \"//dc:date/text()\"\n    pubyears = bibdata.xpath(xpath, namespaces=namespaces)\n\n    # Count the occurrences, find the 10 most frequently mentioned publication dates\n    pubyear_counts = Counter(pubyears)\n    pubyear_counts = dict(sorted(pubyear_counts.items(), reverse=False))\n\n    # Filter data to clean it\n    pubyear_counts = pd.DataFrame.from_dict(pubyear_counts, orient=\"index\").reset_index().rename(mapper={\"index\":\"year\", 0 : \"count\"}, axis=\"columns\")    #pubyear_counts = pubyear_counts[pubyear_counts[0] == 1991]\n    pubyear_counts = pubyear_counts[pubyear_counts[\"year\"].str.isnumeric()]\n    pubyear_counts.set_index(\"year\", inplace=True)\n    # Remove erroneous years (to be corrected in the data)\n    pubyear_counts.drop([\"134\", \"207\", \"22\", \"30\", \"42\", \"58\", \"76\", \"78\", \"20\", \"201\"], inplace=True)\n    # Remove less relevant years (optional, of course)\n    pubyear_counts.drop([\"1815\", \"1834\", \"1891\", \"1932\", \"1945\", \"1957\", \"1961\", \"1969\", \"1973\", \"1974\", \"1975\", \"1978\", \"1979\", \"1981\", \"1982\", \"1983\", \"1984\"], inplace=True)\n    pubyear_counts.reset_index(inplace=True)\n    #print(pubyear_counts.head())\n\n    # Display some key figures\n    print(\"There are \" + str(len(pubyears)) + \" instances of publication years mentioned in the dataset.\")\n    print(\"There are \" + str(len(pubyear_counts)) + \" different years mentioned in the dataset.\")\n\n    return pubyear_counts\n\n\ndef visualize_pubyears(pubyear_counts):\n    plt.figure(figsize=(12,8))\n    pal = sns.color_palette(\"colorblind\", len(pubyear_counts))\n    fig = sns.barplot(\n        data = pubyear_counts, \n        x=\"year\", \n        y=\"count\",\n        palette = pal,\n        )\n    fig.set_xticklabels(fig.get_xticklabels(), rotation=45)\n    plt.savefig(join(wdir, \"figures\", \"pubyear_counts.png\"), dpi=300)\n\n\npubyear_counts = get_pubyears(bibdata)\nvisualize_pubyears(pubyear_counts)\n\n\nThere are 64169 instances of publication years mentioned in the dataset.\nThere are 38 different years mentioned in the dataset."
  },
  {
    "objectID": "titles.html",
    "href": "titles.html",
    "title": "Titles",
    "section": "",
    "text": "The titles included in the bibliography tell us a few things, for instance the language a publication is written in or the main themes (keywords, authors) that is the subject of a publication.\nCode\n# === Imports === \n\nimport re \nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom os.path import join\nfrom os.path import realpath, dirname\nimport os\nfrom lxml import etree\nfrom io import StringIO, BytesIO\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\n\n\n# === Files and parameters === \n\nwdir = join(\"/\", \"media\", \"christof\", \"Data\", \"Github\", \"christofs\", \"BIB18\")\nbibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF.rdf\") \n#bibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF_TEST.rdf\") \nstopwords_fra_file = join(wdir, \"results\", \"stopwords-fra.txt\")\nstopwords_eng_file = join(wdir, \"results\", \"stopwords-eng.txt\")\n\n\nnamespaces = {\n    \"foaf\" : \"http://xmlns.com/foaf/0.1/\",\n    \"bib\" : \"http://purl.org/net/biblio#\",\n    \"dc\" : \"http://purl.org/dc/elements/1.1/\",\n    \"z\" : \"http://www.zotero.org/namespaces/export#\",\n    \"rdf\" : \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n    }\n\n\n# === Load the dataset === \n\ndef read_xml(bibdatafile): \n    bibdata = etree.parse(bibdatafile)\n    return bibdata\nbibdata = read_xml(bibdatafile)"
  },
  {
    "objectID": "titles.html#extracting-the-titles",
    "href": "titles.html#extracting-the-titles",
    "title": "Titles",
    "section": "Extracting the titles",
    "text": "Extracting the titles\nThe first step is to identify the titles in the dataset. At the moment, the primary titles of journal articles, book chapters and books are taken into account.\n\n\nCode\ndef get_titles(bibdata): \n    \"\"\"\n    Extract all the primary titles from the dataset. \n    Primary titles are all titles except journal names. \n    \"\"\"\n    # Find all primary \"title\" elements in the dataset \n    titles = []\n\n    # Article titles\n    xpath = \"//bib:Article/dc:title/text()\"\n    article_titles = bibdata.xpath(xpath, namespaces=namespaces)\n    titles.extend(article_titles)\n\n    # Book titles\n    xpath = \"//bib:Book/dc:title/text()\"\n    book_titles = bibdata.xpath(xpath, namespaces=namespaces)\n    titles.extend(book_titles)\n\n    # Book chapter titles\n    xpath = \"//bib:BookSection/dc:title/text()\"\n    bookchapter_titles = bibdata.xpath(xpath, namespaces=namespaces)\n    titles.extend(bookchapter_titles)\n\n    print(\"Number of titles found: \" + str(len(titles)) + \".\")\n    return titles\n\n# === Main === \n\nglobal titles \ntitles = get_titles(bibdata)\n\n\nNumber of titles found: 62987.\n\n\nThe following step joins all words in all titles, then filters the list using French and English stopwords.\n\n\nCode\ndef load_stopwords(stopwordsfile): \n    with open(stopwordsfile, \"r\", encoding=\"utf8\") as infile: \n        stopwords = infile.read().split(\"\\n\")\n    #print(stopwords)\n    return stopwords \n\n    \ndef get_keywords(titles, stopwords_fra, stopwords_eng): \n    \"\"\"\n    Identify recurring, content-bearing words in the titles. \n    Returns: dict (keyword : frequency)\n    \"\"\"\n    \n    # Filtering of words in all titles\n    titles = \" \".join(titles)\n    titlewords = re.split(\"\\W+\", titles)\n    titlewords = [word.lower() for word in titlewords if word.lower() not in stopwords_fra]\n    titlewords = [word.lower() for word in titlewords if word.lower() not in stopwords_eng]\n    #print(titles[0:20])\n    \n    # Establish counts, transform to DataFrame, sort to select n most frequent words\n    titlewords_counts = dict(Counter(titlewords))\n    titlewords_counts = pd.DataFrame.from_dict(\n        titlewords_counts, \n        orient=\"index\", \n        columns=[\"count\"]).reset_index().rename({\"index\" : \"word\"}, \n        )      \n    titlewords_counts.sort_values(by=\"count\", ascending=False, inplace=True)\n    titlewords_counts = titlewords_counts[0:50]\n   \n    #print(titlewords_counts.head())\n    return titlewords_counts\n\n\nglobal titlewords_counts\nstopwords_fra = load_stopwords(stopwords_fra_file)\nstopwords_eng = load_stopwords(stopwords_eng_file)\ntitlewords_counts = get_keywords(titles, stopwords_fra, stopwords_eng)\n\n\nThe following step provides a visualization of the most frequently used words in the titles.\n\n\nCode\ndef visualize_titlewords(titlewords_counts): \n    plt.figure(figsize=(12,12))\n    pal = sns.color_palette(\"colorblind\", len(titlewords_counts))\n    fig = sns.barplot(\n        data=titlewords_counts, \n        y=\"index\", \n        x=\"count\", \n        orient='h' \n        )\n    for i in fig.containers:\n        fig.bar_label(i,)\n    plt.tight_layout()\n    plt.savefig(\n        join(wdir, \"figures\", \"titlewords_counts.png\"),\n        dpi=600\n        )\n    \nvisualize_titlewords(titlewords_counts)\n\n\n\n\n\nAs can be seen from this table, there are a few authors who appear with particularly high frequency in the titles: Rousseau, Voltaire, Diderot (in descending order).\nIn terms of thematic keywords, apart from indicators of the time period and geographic focus, the following terms stand out (again, in descending order of frequency): histoire, Lumières, révolution, lettres, history, Europe, Enlightenment, art, théâtre, revolution, roman, littérature, correspondance, voyage, culture, etc."
  },
  {
    "objectID": "types.html",
    "href": "types.html",
    "title": "Types",
    "section": "",
    "text": "There are 64397 instances of publication type (each entry has one), with just 6 different types of publication types. This number results from the original bibliography’s data model, but the labels used below are Zotero’s labels. The exception to this rule is that the distinction between monographs and edited volumes is lost in Zotero, which considers both to be books.\n\njournalArticle: 26615\nbook (monographs and edited volumes): 23083\nbookSection: 13368\nthesis: 1296\ndataset: 34\nwebpage: 1\n\n(Visualize as a bubble chart?)\n\n\nCode\n# === Imports === \n\nimport re \nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom os.path import join\nfrom os.path import realpath, dirname\nimport os\nfrom lxml import etree\nfrom io import StringIO, BytesIO\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\n\n\n# === Files and parameters === \n\nwdir = join(\"/\", \"media\", \"christof\", \"Data\", \"Github\", \"christofs\", \"BIB18\")\nbibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF.rdf\") \n#bibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF_TEST.rdf\") \n\nnamespaces = {\n    \"foaf\" : \"http://xmlns.com/foaf/0.1/\",\n    \"bib\" : \"http://purl.org/net/biblio#\",\n    \"dc\" : \"http://purl.org/dc/elements/1.1/\",\n    \"z\" : \"http://www.zotero.org/namespaces/export#\",\n    \"rdf\" : \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n    }\n\n# === Load the dataset === \n\ndef read_xml(bibdatafile): \n    bibdata = etree.parse(bibdatafile)\n    return bibdata\nbibdata = read_xml(bibdatafile)\n\n\n\n\nCode\ndef get_pubtypes(bibdata): \n    # Find all the instances of publication types\n    pubtypes = []\n    xpath = \"//z:itemType/text()\"\n    pubtypes = bibdata.xpath(xpath, namespaces=namespaces)\n    print(\"There are \" + str(len(pubtypes)) +  \" instances of publication type in the dataset.\")\n    return pubtypes\n\n\ndef most_frequent_pubtypes(pubtypes): \n    # Count the occurrences, find the 10 most frequently mentioned persons\n    pubtypes_counts = Counter(pubtypes)\n    print(\"There are \" + str(len(pubtypes_counts)) + \" different publication types in the dataset.\")\n    pubtypes_counts = dict(sorted(pubtypes_counts.items(), key = lambda item: item[1], reverse=True)[:10])\n    columns = [\"counts\"]\n    pubtypes_counts = pd.DataFrame.from_dict(\n        pubtypes_counts, \n        orient=\"index\", \n        columns=[\"count\"]).reset_index().rename({\"index\" : \"types\"}, \n        axis=1\n        )\n    return pubtypes_counts\n\n\nglobal pubtypes_counts\npubtypes = get_pubtypes(bibdata)\npubtypes_counts = most_frequent_pubtypes(pubtypes)\n\n\nThere are 64397 instances of publication type in the dataset.\nThere are 6 different publication types in the dataset."
  },
  {
    "objectID": "types.html#distribution-of-publication-types-in-the-dataset",
    "href": "types.html#distribution-of-publication-types-in-the-dataset",
    "title": "Types",
    "section": "",
    "text": "There are 64397 instances of publication type (each entry has one), with just 6 different types of publication types. This number results from the original bibliography’s data model, but the labels used below are Zotero’s labels. The exception to this rule is that the distinction between monographs and edited volumes is lost in Zotero, which considers both to be books.\n\njournalArticle: 26615\nbook (monographs and edited volumes): 23083\nbookSection: 13368\nthesis: 1296\ndataset: 34\nwebpage: 1\n\n(Visualize as a bubble chart?)\n\n\nCode\n# === Imports === \n\nimport re \nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom os.path import join\nfrom os.path import realpath, dirname\nimport os\nfrom lxml import etree\nfrom io import StringIO, BytesIO\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\n\n\n# === Files and parameters === \n\nwdir = join(\"/\", \"media\", \"christof\", \"Data\", \"Github\", \"christofs\", \"BIB18\")\nbibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF.rdf\") \n#bibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF_TEST.rdf\") \n\nnamespaces = {\n    \"foaf\" : \"http://xmlns.com/foaf/0.1/\",\n    \"bib\" : \"http://purl.org/net/biblio#\",\n    \"dc\" : \"http://purl.org/dc/elements/1.1/\",\n    \"z\" : \"http://www.zotero.org/namespaces/export#\",\n    \"rdf\" : \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n    }\n\n# === Load the dataset === \n\ndef read_xml(bibdatafile): \n    bibdata = etree.parse(bibdatafile)\n    return bibdata\nbibdata = read_xml(bibdatafile)\n\n\n\n\nCode\ndef get_pubtypes(bibdata): \n    # Find all the instances of publication types\n    pubtypes = []\n    xpath = \"//z:itemType/text()\"\n    pubtypes = bibdata.xpath(xpath, namespaces=namespaces)\n    print(\"There are \" + str(len(pubtypes)) +  \" instances of publication type in the dataset.\")\n    return pubtypes\n\n\ndef most_frequent_pubtypes(pubtypes): \n    # Count the occurrences, find the 10 most frequently mentioned persons\n    pubtypes_counts = Counter(pubtypes)\n    print(\"There are \" + str(len(pubtypes_counts)) + \" different publication types in the dataset.\")\n    pubtypes_counts = dict(sorted(pubtypes_counts.items(), key = lambda item: item[1], reverse=True)[:10])\n    columns = [\"counts\"]\n    pubtypes_counts = pd.DataFrame.from_dict(\n        pubtypes_counts, \n        orient=\"index\", \n        columns=[\"count\"]).reset_index().rename({\"index\" : \"types\"}, \n        axis=1\n        )\n    return pubtypes_counts\n\n\nglobal pubtypes_counts\npubtypes = get_pubtypes(bibdata)\npubtypes_counts = most_frequent_pubtypes(pubtypes)\n\n\nThere are 64397 instances of publication type in the dataset.\nThere are 6 different publication types in the dataset."
  },
  {
    "objectID": "types.html#visualization",
    "href": "types.html#visualization",
    "title": "Types",
    "section": "Visualization",
    "text": "Visualization\n\n\nCode\ndef visualize_pubtypes_counts(pubtypes_counts): \n    #print(pubtypes_counts)\n    plt.figure(figsize=(12,6))\n    pal = sns.color_palette(\"colorblind\", len(pubtypes_counts))\n    fig = sns.barplot(\n        data=pubtypes_counts, \n        y=\"types\", \n        x=\"count\", \n        orient='h' \n        )\n    for i in fig.containers:\n        fig.bar_label(i,)\n    plt.tight_layout()\n    plt.savefig(\n        join(wdir, \"figures\", \"pubtypes_counts.png\"),\n        dpi=300\n        )\n    \nvisualize_pubtypes_counts(pubtypes_counts)\n\n\n\n\n\nNote that, unfortunately, Zotero does not distinguish between the item types monograph and edited volume, both being treated as books. (The rule-based disambiguation of these item types, based on the presence of authors and/or editors, is future work.)"
  }
]