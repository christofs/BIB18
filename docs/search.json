[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BIB18",
    "section": "",
    "text": "BIB18 – Curation and analyses of the dataset derived from the Bibliographie sur le XVIIIe siècle"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "BIB18",
    "section": "Overview",
    "text": "Overview\nSome analyses of the dataset are available on the present website. The analyses are based on the RDF version of the data (currently, v0.1.0, May 20, 2023) and have been performed by Christof Schöch. Note that all numbers can be expected to shift slighly as the process of cleaning the data proceeds. Collaborative authorship / editorship as well as the publication types will most likely be most affected by corrections. The scripts to create the analyses and generate the website pages is also available at github.com/christofs/BIB18.\nSource of the data: Benoît Melançon, for background information see mapageweb.umontreal.ca/melancon/donnees_biblios_1_550.html and for the Dataverse deposit, see doi.org/10.5683/SP3/PYYEEH.\nFor more information on the transformation process, and to obtain the dataset in various formats (BibTeX, Zotero RDF, JSON), see: github.com/christofs/BIB18. The transformation was performed in May 2023.\nThe full bibliography can also be consulted on Zotero, see: zotero.org/groups/5067408/BIB18."
  },
  {
    "objectID": "index.html#nota-bene",
    "href": "index.html#nota-bene",
    "title": "BIB18",
    "section": "Nota bene",
    "text": "Nota bene\nBenoît Melançon specifies: “Quiconque souhaite s’approprier ces données peut le faire, sous deux conditions. 1. L’attribution de la collecte des données doit toujours être rappelée, par exemple sous la forme «Données colligées par Benoît Melançon». 2. Aucune exploitation commerciale de ces données n’est tolérée. Elles ne peuvent pas être vendues sous quelque forme que ce soit. Autrement dit, chez Creative Commons : Attribution-NonCommercial 4.0 International (CC BY-NC 4.0) — https://creativecommons.org/licenses/by-nc/4.0/.”"
  },
  {
    "objectID": "index.html#citation-suggestion",
    "href": "index.html#citation-suggestion",
    "title": "BIB18",
    "section": "Citation suggestion",
    "text": "Citation suggestion\nChristof Schöch. BIB18 – Curation and Analysis of the dataset derived from the Bibliographie sur le XVIIIe siècle, v0.2.0. Github.com, 2023. DOI: 10.5281/zenodo.8165485."
  },
  {
    "objectID": "collaborations.html",
    "href": "collaborations.html",
    "title": "Collaborations",
    "section": "",
    "text": "This page focuses on aspects of collaboration, like co-authorship or co-editorship, in the Bibliographie sur le XVIIIe siècle.\nCode\n# === Imports === \n\nimport re \nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom os.path import join, abspath, normpath, realpath\nimport os\nfrom lxml import etree\nfrom io import StringIO, BytesIO\nfrom collections import Counter\nimport pandas as pd\n\n# === Namespaces === \n\nnamespaces = {\n    \"foaf\" : \"http://xmlns.com/foaf/0.1/\",\n    \"bib\" : \"http://purl.org/net/biblio#\",\n    \"dc\" : \"http://purl.org/dc/elements/1.1/\",\n    \"z\" : \"http://www.zotero.org/namespaces/export#\",\n    \"rdf\" : \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n    }\n\n# === Files and parameters === \n\nwdir = join(\"/\", \"media\", \"christof\", \"Data\", \"Github\", \"christofs\", \"BIB18\")\nbibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF.rdf\") \n#bibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF_TEST.rdf\")\nCode\ndef read_xml(bibdatafile): \n    bibdata = etree.parse(bibdatafile)\n    return bibdata\n\nbibdata = read_xml(bibdatafile)"
  },
  {
    "objectID": "collaborations.html#number-of-collaborators",
    "href": "collaborations.html#number-of-collaborators",
    "title": "Collaborations",
    "section": "Number of collaborators",
    "text": "Number of collaborators\nThis part determines how frequent collaborations (co-authorship, co-editorship) are. This is based on the number of “Person” elements within the “authors” or “editors” element.\n\n\nCode\ndef get_number_collaborators(bibdata): \n    \"\"\"\n    Finds out how frequent collaborations (co-authorship, co-editorship) are.\n    Number of \"Person\" elements within \"authors\" or \"editors\" element. \n    \"\"\"\n    print(\"\\nNumber of collaborations with specific number of collaborators.\")\n\n    # Find all instances of authors\n    num_coauthors = []\n    xpath = \"//bib:authors\"\n    authors = bibdata.xpath(xpath, namespaces=namespaces)\n    print(len(authors), \"instances of Element 'authors'\")\n    num_coauthors = []\n    for item in authors:\n        #print(item)\n        xpath = \"rdf:Seq/rdf:li/foaf:Person\"\n        coauthors = item.xpath(xpath, namespaces=namespaces)\n        num_coauthors.append(len(coauthors))\n    num_coauthors_counts = Counter(num_coauthors)\n    print(num_coauthors_counts)\n\n    # Calculate percentages\n    num_coauthors_perc = {}\n    total = sum(num_coauthors_counts.values())\n    for key,val in num_coauthors_counts.items():\n        num_coauthors_perc[key] = str(round(val/total * 100, 3)) + '%'\n    print(num_coauthors_perc)\n\n    # Find all instances of editors\n    num_coeditors = []\n    xpath = \"//bib:editors\"\n    editors = bibdata.xpath(xpath, namespaces=namespaces)\n    print(len(editors), \"instances of Element 'editors'\")\n    num_coeditors = []\n    for item in editors:\n        xpath = \"rdf:Seq/rdf:li/foaf:Person\"\n        coeditors = item.xpath(xpath, namespaces=namespaces)\n        num_coeditors.append(len(coeditors))\n    num_coeditors_counts = Counter(num_coeditors)\n    print(dict(num_coeditors_counts))\n\n    # Calculate percentages\n    num_coeditors_perc = {}\n    total = sum(num_coeditors_counts.values())\n    for key,val in num_coeditors_counts.items():\n        num_coeditors_perc[key] = str(round(val/total * 100, 2)) + '%'\n    print(num_coeditors_perc)\n\nget_number_collaborators(bibdata)\n\n\n\nNumber of collaborations with specific number of collaborators.\n56860 instances of Element 'authors'\nCounter({1: 53488, 2: 3361, 3: 9, 4: 2})\n{1: '94.07%', 2: '5.911%', 3: '0.016%', 4: '0.004%'}\n17135 instances of Element 'editors'\n{1: 5558, 4: 904, 3: 2638, 2: 7743, 5: 256, 6: 35, 11: 1}\n{1: '32.44%', 4: '5.28%', 3: '15.4%', 2: '45.19%', 5: '1.49%', 6: '0.2%', 11: '0.01%'}"
  },
  {
    "objectID": "collaborations.html#collaborator-networks",
    "href": "collaborations.html#collaborator-networks",
    "title": "Collaborations",
    "section": "Collaborator networks",
    "text": "Collaborator networks\nBuilds the data for a network of people having collaborated as editors.\n\n\nCode\ncoeditorcounts_top_file = join(wdir, \"results\", \"coeditor-counts_top.csv\")\ncoeditorcounts_full_file = join(wdir, \"results\", \"coeditor-counts_full.csv\")\n\n\ndef network_coeditors(bibdata): \n    \"\"\"\n    Builds the data for a network of people having collaborated as editors. \n    \"\"\"\n    # Find all instances of editors\n    xpath = \"//bib:editors\"\n    editors = bibdata.xpath(xpath, namespaces=namespaces)\n    print(\"There are \" + str(len(editors)) + \" editors (i.e., instances of Element 'editors').\")\n\n    # Collect the names of each person within each editors element\n    coeditors = []\n    for item in editors:\n        xpath = \"rdf:Seq/rdf:li/foaf:Person\"\n        coeditors_elements = item.xpath(xpath, namespaces=namespaces)\n        coeditors_names = []\n        # Get the names (full name or first name, last name) from each person\n        for item in coeditors_elements: \n            if len(item) == 2: \n                coeditors_names.append(item[0].text + \", \" + item[1].text)\n        coeditors.append(coeditors_names)\n\n    # Establish the count of each collaboration between editors\n    import itertools \n    all_coeditor_combinations = []\n    for item in coeditors: \n        coeditor_combinations = list(itertools.combinations(item, 2))\n        coeditor_combinations = [tuple(sorted(item)) for item in coeditor_combinations]\n        for coedcomb in coeditor_combinations: \n            all_coeditor_combinations.append(coedcomb)\n    ccc = dict(Counter(all_coeditor_combinations)) # ccc = coeditor_combinations_count\n\n    # Transform to a DataFrame\n    ccc = pd.DataFrame.from_dict(ccc, orient=\"index\", columns=[\"count\"])\n    ccc = ccc.reset_index()\n    ccc_split = pd.DataFrame(ccc[\"index\"].tolist())\n    ccc_merged = ccc_split.merge(ccc, left_index=True, right_index=True)\n    ccc = ccc_merged.drop([\"index\"], axis=1)\n    ccc = ccc.rename({0 : \"coeditor1\", 1 : \"coeditor2\"}, axis=1)\n    ccc = ccc.sort_values(by=\"count\", ascending=False)\n    #print(ccc.head())\n    #print(ccc.shape, \"shape of dataframe\")\n    with open(join(coeditorcounts_full_file), \"w\", encoding=\"utf8\") as outfile: \n        ccc.to_csv(outfile, sep=\";\")\n\n    # Filter the DataFrame to make it manageable for visualization\n    # Determine the top N most frequent co-editors\n    coeditors_top = list(set(list(ccc.head(20).loc[:,\"coeditor1\"]) +\\\n        list(ccc.head(20).loc[:,\"coeditor2\"])))\n    #print(coeditors_top)\n    print(\"Among all editors, \" + str(len(coeditors_top)) + \" have been selected as the most active co-editors.\")\n    # Filter the DataFrame to include just the collaborations involving at least one of the top co-editors. \n    # The resulting DataFrame will have all collaborations between the top co-editors and their co-editors. \n    ccc_filtered = ccc[(ccc[\"coeditor1\"].isin(coeditors_top)) |\\\n                       (ccc[\"coeditor2\"].isin(coeditors_top))]\n    #print(ccc_filtered.shape, \"shape of dataframe of top co-editors and their co-editors.\")\n    # Simplify the labels \n    #ccc_filtered = ccc_filtered.replace(' .*?]', '',regex=True).astype(str)\n    ccc_filtered.loc[:,'coeditor1'] =  [re.sub(r', .*','', str(x)) for x in ccc_filtered.loc[:,'coeditor1']]\n    ccc_filtered.loc[:,'coeditor2'] =  [re.sub(r', .*','', str(x)) for x in ccc_filtered.loc[:,'coeditor2']]\n    print(\"The following table shows the 5 most active pairs of editors.\\n\")\n    print(ccc_filtered.head())\n    with open(join(coeditorcounts_top_file), \"w\", encoding=\"utf8\") as outfile: \n        ccc_filtered.to_csv(outfile, sep=\";\")\n\nnetwork_coeditors(bibdata)"
  },
  {
    "objectID": "collaborations.html#prevalence-of-collaboration-in-the-dataset",
    "href": "collaborations.html#prevalence-of-collaboration-in-the-dataset",
    "title": "Collaborations",
    "section": "Prevalence of collaboration in the dataset",
    "text": "Prevalence of collaboration in the dataset\nThere are 56860 publications in the bibliography that have an author role (for instance monographs, journal articles and book chapters). Shown here is the number of authors in each of these publications. Single authorship is the norm, dual authorship is not uncommon, anything beyond this is exceedingly rare.\n\n1 author: 53488 (94.1%)\n2 authors: 3361 (5.9%)\n3 authors: 9 (0.02 %)\n4 authors: 2 (0.004%)\n\nThere are 17135 publications in the bibliography that have an editor role (for instance edited volumes, proceedings, special issues, textual editions, etc.). Shown here is the number of editors in each of these publications. Dual editorship is the most common case, but single editorship and triple editorship are also very common. Higher numbers of editors are rarer.\n\n1 editor: 5558 (32.4%)\n2 editors: 7743 (45.2%)\n3 editors: 2638 (15.4%)\n4 editors: 904 (5.3%)\n5 editors: 256 (1.5%)\n6 editors: 35 (0.2%)\n11 (!) editors: (0.01%)"
  },
  {
    "objectID": "collaborations.html#co-editor-networks",
    "href": "collaborations.html#co-editor-networks",
    "title": "Collaborations",
    "section": "Co-editor networks",
    "text": "Co-editor networks\nAs shown above, editorship is an area of particularly intense collaboration in the community of Dix-huitiémistes, based on the data in the bibliography. The following is an initial, experimental attempt to elucidate the data using network visualization.\nThe following shows a network of the top 20 co-editors and all of their co-editors, resulting in 148 different co-editor pairs. Each node is one editor, and each time two people have co-edited a publication, a link between them is created. The more co-editorships a person accumulates, the larger the node. The more co-editorships two people share, the thicker the edge connecting them. For this visualization, the full data of collaborations for edited volumes and editions has been massively reduced. Different parameters may strongly affect the results. See the full coeditor data in the data folder for more details.\n\n\n\nNetwork showing the top 20 co-editors and all of their co-editors, created using Gephi.\n\n\nThis figure is available also as a zoom-able image file and with somewhat friendlier colors but no community detection.\nWe basically see three key co-editor networks (the different colors are based on an algorithmic community or cluster detection):\n\nPorret, Rosset, Majeur, Farkas, Baczko et al. \nSermain, Herman, Pelckmans, Escola, Omacini, Peeters, Paschoud, Berchtold et al. \nLeuwers, Bourdin, Biard, Simien, Serna, Antoine et al. \nSmaller clusters with Didier and Neefs as well around Kolving and Mortier.\n\nSome initial observations: While Porret appears to be the most productive co-editor overall, this is achived with some frequent, but also with a large number of less frequent coeditors. Inversely, the most intense collaboration appears to be between Herman and Pelckmans, who seem to avoid one-off collaborations. Finally, Rosset also functions as a bridge linking Porret and Baczko on the one hand, and Herman and Pelckmans on the other hand, and their respective coeditor networks. No similar bridge exists towards the Leuwers et al. network. The smaller Didier cluster is also connected to Sermain."
  },
  {
    "objectID": "languages.html",
    "href": "languages.html",
    "title": "Languages",
    "section": "",
    "text": "This section provides some analyses of the languages of publications referenced in the Bibliographie.\n\n\nCode\n# === Imports === \n\nimport re \nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom os.path import join\nfrom os.path import realpath, dirname\nimport os\nfrom lxml import etree\nfrom io import StringIO, BytesIO\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\n\n\n# === Files and parameters === \n\nwdir = join(\"/\", \"media\", \"christof\", \"Data\", \"Github\", \"christofs\", \"BIB18\")\nbibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF.rdf\") \n#bibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF_TEST.rdf\") \n\nnamespaces = {\n    \"foaf\" : \"http://xmlns.com/foaf/0.1/\",\n    \"bib\" : \"http://purl.org/net/biblio#\",\n    \"dc\" : \"http://purl.org/dc/elements/1.1/\",\n    \"z\" : \"http://www.zotero.org/namespaces/export#\",\n    \"rdf\" : \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n    }\n\n# === Load the dataset === \n\ndef read_xml(bibdatafile): \n    bibdata = etree.parse(bibdatafile)\n    return bibdata\nbibdata = read_xml(bibdatafile)\n\n\n\n\nCode\ndef get_languages(bibdata): \n    print(\"\\nLanguages\")\n\n    # Find all the instances of \"language\" Element and its content\n    xpath = \"//z:language/text()\"\n    languages = bibdata.xpath(xpath, namespaces=namespaces)\n    # Identify frequency of languages\n    languages_counts = Counter(languages)\n    languages_counts = dict(sorted(languages_counts.items(), key = lambda item: item[1], reverse=True)[:10])\n    \n \n    # Visualize using a simple bar chart \n    lc = pd.DataFrame.from_dict(languages_counts, orient=\"index\", columns=[\"count\"]).reset_index().rename({\"index\" : \"language\"}, axis=1)\n    plt.figure(figsize=(12,6))\n    pal = sns.color_palette(\"colorblind\", len(lc))\n    fig = sns.barplot(data=lc, x=\"language\", y=\"count\", palette=pal)\n    fig.set_xticklabels(fig.get_xticklabels(), rotation=30)\n    for i in fig.containers:\n        fig.bar_label(i,)\n    plt.tight_layout()\n    plt.savefig(join(wdir, \"figures\", \"languages_counts.png\"), dpi=300)\n    return languages, languages_counts\n  \nlanguages, languages_counts = get_languages(bibdata)\n\n\n\n\nLanguages\n\n\n\n\n\n\n\nCode\n    \n# Provide some results as a text. \nprint(\"There are \" + str(len(languages))  + \" instances of language in the dataset.\")\nprint(\"At the moment, only \" + str(len(languages_counts)) + \" different languages are considered for analysis.\")\nlanguages_perc = {k: v / len(languages) for k, v in languages_counts.items()}\nprint(\"The most prevalent language is \" + str(list(languages_perc.keys())[0]) + \", with \" + \"{:2.2%}\".format(list(languages_perc.values())[0]) + \" of all entries.\")\n\n\n\n\nThere are 64396 instances of language in the dataset.\nAt the moment, only 8 different languages are considered for analysis.\nThe most prevalent language is French, with 73.89% of all entries."
  },
  {
    "objectID": "people.html",
    "href": "people.html",
    "title": "People",
    "section": "",
    "text": "This page provides information about person names, i.e. authors and editors.\n\n\nCode\n# === Imports === \n\nimport re \nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom os.path import join\nfrom os.path import realpath, dirname\nimport os\nfrom lxml import etree\nfrom io import StringIO, BytesIO\nfrom collections import Counter\nimport pandas as pd\n\n\n# === Files and parameters === \n\nwdir = join(\"/\", \"media\", \"christof\", \"Data\", \"Github\", \"christofs\", \"BIB18\")\nbibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF.rdf\") \n#bibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF_TEST.rdf\") \n\n\nnamespaces = {\n    \"foaf\" : \"http://xmlns.com/foaf/0.1/\",\n    \"bib\" : \"http://purl.org/net/biblio#\",\n    \"dc\" : \"http://purl.org/dc/elements/1.1/\",\n    \"z\" : \"http://www.zotero.org/namespaces/export#\",\n    \"rdf\" : \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n    }\n\n\ndef read_json(bibdatafile): \n    bibdata = etree.parse(bibdatafile)\n    return bibdata\n\nbibdata = read_json(bibdatafile)\n\n\n\n\nCode\ndef get_personnames(bibdata): \n\n    # Find all the instances of persons\n    personnames = []\n    xpath = \"//foaf:Person\"\n    persons = bibdata.xpath(xpath, namespaces=namespaces)\n    print(\"There are \" + str(len(persons)) + \" instances of the Element 'person' in the dataset.\")\n\n    # Get the names (full name or first name, last name) from each person\n    for item in persons: \n        if len(item) == 1: \n            personname = item[0].text\n            personnames.append(personname)\n        elif len(item) == 2: \n            personname = item[0].text + \", \" + item[1].text \n            personnames.append(personname)    \n    print(\"There are \" + str(len(Counter(personnames))) + \" different person names in the dataset.\")\n    return personnames\n\nglobal personnames\npersonnames  = get_personnames(bibdata)\n\n\nThere are 94321 instances of the Element 'person' in the dataset.\nThere are 31026 different person names in the dataset."
  },
  {
    "objectID": "people.html#most-frequently-occurring-person-names",
    "href": "people.html#most-frequently-occurring-person-names",
    "title": "People",
    "section": "",
    "text": "This page provides information about person names, i.e. authors and editors.\n\n\nCode\n# === Imports === \n\nimport re \nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom os.path import join\nfrom os.path import realpath, dirname\nimport os\nfrom lxml import etree\nfrom io import StringIO, BytesIO\nfrom collections import Counter\nimport pandas as pd\n\n\n# === Files and parameters === \n\nwdir = join(\"/\", \"media\", \"christof\", \"Data\", \"Github\", \"christofs\", \"BIB18\")\nbibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF.rdf\") \n#bibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF_TEST.rdf\") \n\n\nnamespaces = {\n    \"foaf\" : \"http://xmlns.com/foaf/0.1/\",\n    \"bib\" : \"http://purl.org/net/biblio#\",\n    \"dc\" : \"http://purl.org/dc/elements/1.1/\",\n    \"z\" : \"http://www.zotero.org/namespaces/export#\",\n    \"rdf\" : \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n    }\n\n\ndef read_json(bibdatafile): \n    bibdata = etree.parse(bibdatafile)\n    return bibdata\n\nbibdata = read_json(bibdatafile)\n\n\n\n\nCode\ndef get_personnames(bibdata): \n\n    # Find all the instances of persons\n    personnames = []\n    xpath = \"//foaf:Person\"\n    persons = bibdata.xpath(xpath, namespaces=namespaces)\n    print(\"There are \" + str(len(persons)) + \" instances of the Element 'person' in the dataset.\")\n\n    # Get the names (full name or first name, last name) from each person\n    for item in persons: \n        if len(item) == 1: \n            personname = item[0].text\n            personnames.append(personname)\n        elif len(item) == 2: \n            personname = item[0].text + \", \" + item[1].text \n            personnames.append(personname)    \n    print(\"There are \" + str(len(Counter(personnames))) + \" different person names in the dataset.\")\n    return personnames\n\nglobal personnames\npersonnames  = get_personnames(bibdata)\n\n\nThere are 94321 instances of the Element 'person' in the dataset.\nThere are 31026 different person names in the dataset."
  },
  {
    "objectID": "people.html#visualization-of-the-most-frequent-person-names",
    "href": "people.html#visualization-of-the-most-frequent-person-names",
    "title": "People",
    "section": "Visualization of the most frequent person names",
    "text": "Visualization of the most frequent person names\nThese persons could be authors or editors of publications.\n\n\nCode\ndef most_frequent_persons(personnames):\n    # Count the occurrences, find the 10 most frequently mentioned publishers\n    personnames_counts = Counter(personnames)\n    personnames_counts = dict(\n        sorted(personnames_counts.items(), \n        key = lambda item: item[1], reverse=True)[:20]\n        )\n    \n    columns = [\"count\"]\n    personnames_counts = pd.DataFrame.from_dict(\n        personnames_counts, \n        orient=\"index\", \n        columns=[\"count\"]).reset_index().rename({\"index\" : \"person\"}, \n        axis=1\n        )\n    \n    #print(personnames_counts)\n    return personnames_counts\n\n\ndef visualize_personnames_counts(personnames_counts): \n    #print(publishernames_counts)\n    plt.figure(figsize=(12,8))\n    pal = sns.color_palette(\"colorblind\", len(personnames_counts))\n    fig = sns.barplot(\n        data=personnames_counts, \n        y=\"person\", \n        x=\"count\", \n        orient='h' \n        )\n    for i in fig.containers:\n        fig.bar_label(i,)\n    plt.tight_layout()\n    plt.savefig(\n        join(wdir, \"figures\", \"personnames_counts.png\"),\n        dpi=300\n        )\n\n\npersonnames_counts = most_frequent_persons(personnames)\nvisualize_personnames_counts(personnames_counts)\n\n\nThere are 31026 different person names in the dataset."
  },
  {
    "objectID": "publishers.html",
    "href": "publishers.html",
    "title": "Publishers",
    "section": "",
    "text": "This page provides information about the publishers mentioned in the dataset.\nCode\n# === Imports === \n\nimport re \nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom os.path import join\nfrom os.path import realpath, dirname\nimport os\nfrom lxml import etree\nfrom io import StringIO, BytesIO\nfrom collections import Counter\nimport pandas as pd\n\n\n# === Files and parameters === \n\nwdir = join(\"/\", \"media\", \"christof\", \"Data\", \"Github\", \"christofs\", \"BIB18\")\nbibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF.rdf\") \n#bibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF_TEST.rdf\") \n\n\nnamespaces = {\n    \"foaf\" : \"http://xmlns.com/foaf/0.1/\",\n    \"bib\" : \"http://purl.org/net/biblio#\",\n    \"dc\" : \"http://purl.org/dc/elements/1.1/\",\n    \"z\" : \"http://www.zotero.org/namespaces/export#\",\n    \"rdf\" : \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n    }\n\n\ndef read_json(bibdatafile): \n    bibdata = etree.parse(bibdatafile)\n    return bibdata\n\nbibdata = read_json(bibdatafile)\nCode\ndef get_publishers(bibdata): \n    # Find all the instances of publisher names\n    publishers = []\n    xpath = \"//dc:publisher//foaf:name/text()\"\n    publishers = bibdata.xpath(xpath, namespaces=namespaces)    \n    # Show some results\n    print(\"There are \" + str(len(publishers)) + \" instances of publishers mentioned in the dataset.\" )\n    print(\"There are a total of \" + str(len(set(publishers))) + \" different publishers mentioned in the dataset.\" )\n    return publishers\n\n\ndef most_frequent_publishers(publishers):\n    # Count the occurrences, find the 10 most frequently mentioned publishers\n    publishernames_counts = Counter(publishers)\n    publishernames_counts = dict(\n        sorted(publishernames_counts.items(), \n        key = lambda item: item[1], reverse=True)[:20]\n        )\n    \n    columns = [\"occurrences\"]\n    publishernames_counts = pd.DataFrame.from_dict(\n        publishernames_counts, \n        orient=\"index\", \n        columns=[\"count\"]).reset_index().rename({\"index\" : \"publisher\"}, \n        axis=1\n        )\n    return publishernames_counts\n\n\nglobal publishernames_counts\npublishers = get_publishers(bibdata)\npublishernames_counts = most_frequent_publishers(publishers)\n\n\nThere are 37781 instances of publishers mentioned in the dataset.\nThere are a total of 5705 different publishers mentioned in the dataset."
  },
  {
    "objectID": "time.html",
    "href": "time.html",
    "title": "Time",
    "section": "",
    "text": "Here, only data starting in 1986 is shown (the bibliography was launched in 1992).\n\n\nCode\n# === Imports === \n\nimport re \nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom os.path import join\nfrom os.path import realpath, dirname\nimport os\nfrom lxml import etree\nfrom io import StringIO, BytesIO\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\n\n\n# === Files and parameters === \n\nwdir = join(\"/\", \"media\", \"christof\", \"Data\", \"Github\", \"christofs\", \"BIB18\")\nbibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF.rdf\") \n#bibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF_TEST.rdf\") \n\nnamespaces = {\n    \"foaf\" : \"http://xmlns.com/foaf/0.1/\",\n    \"bib\" : \"http://purl.org/net/biblio#\",\n    \"dc\" : \"http://purl.org/dc/elements/1.1/\",\n    \"z\" : \"http://www.zotero.org/namespaces/export#\",\n    \"rdf\" : \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n    }\n\n# === Load the dataset === \n\ndef read_xml(bibdatafile): \n    bibdata = etree.parse(bibdatafile)\n    return bibdata\nbibdata = read_xml(bibdatafile)\n\n\n\n\nCode\n\ndef get_pubyears(bibdata): \n    # Setting things up\n    pubyears = []\n\n    # Find all the instances of publication dates\n    xpath = \"//dc:date/text()\"\n    pubyears = bibdata.xpath(xpath, namespaces=namespaces)\n\n    # Count the occurrences, find the 10 most frequently mentioned publication dates\n    pubyear_counts = Counter(pubyears)\n    pubyear_counts = dict(sorted(pubyear_counts.items(), reverse=False))\n\n    # Filter data to clean it\n    pubyear_counts = pd.DataFrame.from_dict(pubyear_counts, orient=\"index\").reset_index().rename(mapper={\"index\":\"year\", 0 : \"count\"}, axis=\"columns\")    #pubyear_counts = pubyear_counts[pubyear_counts[0] == 1991]\n    pubyear_counts = pubyear_counts[pubyear_counts[\"year\"].str.isnumeric()]\n    pubyear_counts.set_index(\"year\", inplace=True)\n    # Remove erroneous years (to be corrected in the data)\n    pubyear_counts.drop([\"134\", \"207\", \"22\", \"30\", \"42\", \"58\", \"76\", \"78\", \"20\", \"201\"], inplace=True)\n    # Remove less relevant years (optional, of course)\n    pubyear_counts.drop([\"1815\", \"1834\", \"1891\", \"1932\", \"1945\", \"1957\", \"1961\", \"1969\", \"1973\", \"1974\", \"1975\", \"1978\", \"1979\", \"1981\", \"1982\", \"1983\", \"1984\"], inplace=True)\n    pubyear_counts.reset_index(inplace=True)\n    #print(pubyear_counts.head())\n\n    # Display some key figures\n    print(\"There are \" + str(len(pubyears)) + \" instances of publication years mentioned in the dataset.\")\n    print(\"There are \" + str(len(pubyear_counts)) + \" different years mentioned in the dataset.\")\n\n    return pubyear_counts\n\n\ndef visualize_pubyears(pubyear_counts):\n    plt.figure(figsize=(12,8))\n    pal = sns.color_palette(\"colorblind\", len(pubyear_counts))\n    fig = sns.barplot(\n        data = pubyear_counts, \n        x=\"year\", \n        y=\"count\",\n        palette = pal,\n        )\n    fig.set_xticklabels(fig.get_xticklabels(), rotation=45)\n    plt.savefig(join(wdir, \"figures\", \"pubyear_counts.png\"), dpi=300)\n\n\npubyear_counts = get_pubyears(bibdata)\nvisualize_pubyears(pubyear_counts)\n\n\nThere are 64169 instances of publication years mentioned in the dataset.\nThere are 38 different years mentioned in the dataset."
  },
  {
    "objectID": "time.html#distribution-of-publications-per-year",
    "href": "time.html#distribution-of-publications-per-year",
    "title": "Time",
    "section": "",
    "text": "Here, only data starting in 1986 is shown (the bibliography was launched in 1992).\n\n\nCode\n# === Imports === \n\nimport re \nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom os.path import join\nfrom os.path import realpath, dirname\nimport os\nfrom lxml import etree\nfrom io import StringIO, BytesIO\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\n\n\n# === Files and parameters === \n\nwdir = join(\"/\", \"media\", \"christof\", \"Data\", \"Github\", \"christofs\", \"BIB18\")\nbibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF.rdf\") \n#bibdatafile = join(wdir, \"data\", \"BIB18_Zotero-RDF_TEST.rdf\") \n\nnamespaces = {\n    \"foaf\" : \"http://xmlns.com/foaf/0.1/\",\n    \"bib\" : \"http://purl.org/net/biblio#\",\n    \"dc\" : \"http://purl.org/dc/elements/1.1/\",\n    \"z\" : \"http://www.zotero.org/namespaces/export#\",\n    \"rdf\" : \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n    }\n\n# === Load the dataset === \n\ndef read_xml(bibdatafile): \n    bibdata = etree.parse(bibdatafile)\n    return bibdata\nbibdata = read_xml(bibdatafile)\n\n\n\n\nCode\n\ndef get_pubyears(bibdata): \n    # Setting things up\n    pubyears = []\n\n    # Find all the instances of publication dates\n    xpath = \"//dc:date/text()\"\n    pubyears = bibdata.xpath(xpath, namespaces=namespaces)\n\n    # Count the occurrences, find the 10 most frequently mentioned publication dates\n    pubyear_counts = Counter(pubyears)\n    pubyear_counts = dict(sorted(pubyear_counts.items(), reverse=False))\n\n    # Filter data to clean it\n    pubyear_counts = pd.DataFrame.from_dict(pubyear_counts, orient=\"index\").reset_index().rename(mapper={\"index\":\"year\", 0 : \"count\"}, axis=\"columns\")    #pubyear_counts = pubyear_counts[pubyear_counts[0] == 1991]\n    pubyear_counts = pubyear_counts[pubyear_counts[\"year\"].str.isnumeric()]\n    pubyear_counts.set_index(\"year\", inplace=True)\n    # Remove erroneous years (to be corrected in the data)\n    pubyear_counts.drop([\"134\", \"207\", \"22\", \"30\", \"42\", \"58\", \"76\", \"78\", \"20\", \"201\"], inplace=True)\n    # Remove less relevant years (optional, of course)\n    pubyear_counts.drop([\"1815\", \"1834\", \"1891\", \"1932\", \"1945\", \"1957\", \"1961\", \"1969\", \"1973\", \"1974\", \"1975\", \"1978\", \"1979\", \"1981\", \"1982\", \"1983\", \"1984\"], inplace=True)\n    pubyear_counts.reset_index(inplace=True)\n    #print(pubyear_counts.head())\n\n    # Display some key figures\n    print(\"There are \" + str(len(pubyears)) + \" instances of publication years mentioned in the dataset.\")\n    print(\"There are \" + str(len(pubyear_counts)) + \" different years mentioned in the dataset.\")\n\n    return pubyear_counts\n\n\ndef visualize_pubyears(pubyear_counts):\n    plt.figure(figsize=(12,8))\n    pal = sns.color_palette(\"colorblind\", len(pubyear_counts))\n    fig = sns.barplot(\n        data = pubyear_counts, \n        x=\"year\", \n        y=\"count\",\n        palette = pal,\n        )\n    fig.set_xticklabels(fig.get_xticklabels(), rotation=45)\n    plt.savefig(join(wdir, \"figures\", \"pubyear_counts.png\"), dpi=300)\n\n\npubyear_counts = get_pubyears(bibdata)\nvisualize_pubyears(pubyear_counts)\n\n\nThere are 64169 instances of publication years mentioned in the dataset.\nThere are 38 different years mentioned in the dataset."
  },
  {
    "objectID": "titles.html",
    "href": "titles.html",
    "title": "Titles",
    "section": "",
    "text": "The titles included in the bibliography tell us a few things, for instance the language a publication is written in or the main themes (keywords, authors) that is the subject of a publication. Let’s find out about them. (The code below initializes the analyis.)\nCode\n# === Imports === \n\n# Basics\nimport re \nfrom os.path import realpath, dirname, join\nimport os\nfrom lxml import etree\nfrom collections import Counter\nimport pandas as pd\n\n# Visualization\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\nimport plotly.express as px\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n\n# === Files and parameters === \n\nwdir  = join(\"/\", \"media\", \"christof\", \"Data\", \"Github\", \"christofs\", \"BIB18\")\nBib18_file = join(wdir, \"data\", \"BIB18_Zotero-RDF.rdf\") \n#Bib18_file = join(wdir, \"data\", \"BIB18_Zotero-RDF_TEST.rdf\") \n\nnamespaces = {\n    \"foaf\" : \"http://xmlns.com/foaf/0.1/\",\n    \"bib\" : \"http://purl.org/net/biblio#\",\n    \"dc\" : \"http://purl.org/dc/elements/1.1/\",\n    \"z\" : \"http://www.zotero.org/namespaces/export#\",\n    \"rdf\" : \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n    }\n\ndef read_rdf(Bib18_file): \n    \"\"\"\n    Open and read the RDF version of the Bib18 dataset.\n    Returns: the XML document as an etree object. \n    \"\"\"\n    Bib18 = etree.parse(Bib18_file)\n    return Bib18\n\n# === Main ===\n\nglobal Bib18\nBib18 = read_rdf(Bib18_file)"
  },
  {
    "objectID": "titles.html#extracting-the-titles",
    "href": "titles.html#extracting-the-titles",
    "title": "Titles",
    "section": "Extracting the titles",
    "text": "Extracting the titles\nThe first step is to identify the titles in the dataset. Ideally, we would exclude titles of journals, for instance, but this is not done here yet.\n\n\nCode\ndef get_titles(Bib18): \n    \"\"\"\n    Extract all the primary titles from the dataset. \n    Primary titles are all titles except journal names. \n    \"\"\"\n    # Find all primary \"title\" elements in the dataset \n    # TODO: find the XPath to exclude journal titles. \n    xpath = \"//dc:title/text()\"\n    titles = Bib18.xpath(xpath, namespaces=namespaces)\n    print(\"Number of titles found: \" + str(len(titles)) + \".\")\n    return titles\n\n# === Main === \n\nglobal titles \ntitles = get_titles(Bib18)\n\n\nNumber of titles found: 110174."
  },
  {
    "objectID": "titles.html#distribution-of-languages-based-on-the-titles",
    "href": "titles.html#distribution-of-languages-based-on-the-titles",
    "title": "Titles",
    "section": "Distribution of languages, based on the titles",
    "text": "Distribution of languages, based on the titles\nWe don’t have abstracts or full texts in the dataset, but on the basis of the titles alone, the (most likely) language of the publication can be determined. (Note that an algorithmic process, based on the library py-lingua and using only the sometimes very short titles, has been used to create this data, so errors are to be expected. With the progress of corrections in the dataset, this will improve over time.)\nThe following table shows the number of times several different languages occur in the dataset.\n\n\nCode\ndef get_lang_counts(Bib18): \n    \"\"\"\n    Extract all the language elements from the dataset. \n    Then, establish the counts of the languages. \n    \"\"\"\n    # Find all \"language\" elements in the dataset. \n    xpath = \"//z:language/text()\"\n    langs = Bib18.xpath(xpath, namespaces=namespaces)\n    print(\"Number of language indications found: \" + str(len(langs)) + \".\")\n\n    # Establish the counts of each language\n    lang_counts = dict(Counter(langs))\n    lang_counts = pd.DataFrame.from_dict(lang_counts, orient=\"index\").reset_index().rename(mapper={\"index\":\"language\", 0 : \"count\"}, axis=\"columns\")\n    lang_counts.sort_values(by=\"count\", ascending=False, inplace=True)\n\n    # Display the table nicely\n    display(lang_counts.head(10).style.hide(axis=\"index\"))\n\n    # Save the data to a CSV file (as runtime is long)\n    with open(join(wdir, \"data\", \"lang_counts.csv\"), \"w\", encoding=\"utf8\") as outfile: \n        lang_counts.to_csv(outfile)\n\n    # Return data\n    return lang_counts\n\nglobal lang_counts\nlang_counts = get_lang_counts(Bib18)\n\n\nNumber of language indications found: 64396.\n\n\n\n\n\n\n\nlanguage\ncount\n\n\n\n\nFrench\n47579\n\n\nEnglish\n13348\n\n\nItalian\n958\n\n\nGerman\n915\n\n\nSpanish\n866\n\n\nDutch\n282\n\n\nSwedish\n231\n\n\nPortuguese\n217\n\n\n\n\n\nLet’s visualize this data as well.\n\n\nCode\ndef visualize_tlc(lang_counts): \n    \"\"\"\n    Create a visualization (barplot) of the languages counts. \n    \"\"\"\n    #fig = px.bar(\n    #    lang_counts.head(10), \n    #    x=\"language\", \n    #    y=\"count\", \n    #    title=\"Distribution of languages in the titles\",\n    #    text_auto=True)\n    #fig.show(renderer=\"notebook\")\n\n    plt.figure(figsize=(12,6))\n    fig = sns.barplot(\n        data=lang_counts.head(10), \n        x=\"language\", \n        y=\"count\")\n    fig.set_xticklabels(fig.get_xticklabels(), rotation=30)\n    plt.savefig(join(wdir, \"figures\", \"language_counts.svg\"))\n    plt.show()\n\nvisualize_tlc(lang_counts)\n\n\n\n\n\nThe dominance of French is very clear, followed by English. As noted above, this analysis will be strongly improved with manual corrections to the language information."
  },
  {
    "objectID": "types.html",
    "href": "types.html",
    "title": "Types",
    "section": "",
    "text": "There are 64397 instances of publication type (each entry has one), with just 6 different types of publication types. This number results from the original bibliography’s data model, but the labels used below are Zotero’s labels. The exception to this rule is that the distinction between monographs and edited volumes is lost in Zotero, which considers both to be books.\n\njournalArticle: 26615\nbook (monographs and edited volumes): 23083\nbookSection: 13368\nthesis: 1296\ndataset: 34\nwebpage: 1\n\n(Visualize as a bubble chart?)"
  },
  {
    "objectID": "types.html#distribution-of-publication-types-in-the-dataset",
    "href": "types.html#distribution-of-publication-types-in-the-dataset",
    "title": "Types",
    "section": "",
    "text": "There are 64397 instances of publication type (each entry has one), with just 6 different types of publication types. This number results from the original bibliography’s data model, but the labels used below are Zotero’s labels. The exception to this rule is that the distinction between monographs and edited volumes is lost in Zotero, which considers both to be books.\n\njournalArticle: 26615\nbook (monographs and edited volumes): 23083\nbookSection: 13368\nthesis: 1296\ndataset: 34\nwebpage: 1\n\n(Visualize as a bubble chart?)"
  },
  {
    "objectID": "publishers.html#visualization",
    "href": "publishers.html#visualization",
    "title": "Publishers",
    "section": "Visualization",
    "text": "Visualization\n\n\nCode\n\ndef visualize_publishername_counts(publishernames_counts): \n    #print(publishernames_counts)\n    plt.figure(figsize=(12,8))\n    pal = sns.color_palette(\"colorblind\", len(publishernames_counts))\n    fig = sns.barplot(\n        data=publishernames_counts, \n        y=\"publisher\", \n        x=\"count\", \n        orient='h' \n        )\n    for i in fig.containers:\n        fig.bar_label(i,)\n    plt.tight_layout()\n    plt.savefig(\n        join(wdir, \"figures\", \"publishernames_counts.png\"),\n        dpi=300\n        )\n    \nvisualize_publishername_counts(publishernames_counts)"
  }
]